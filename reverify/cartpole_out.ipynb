{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amelia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole_nnet():\n",
    "    #from test import CartPoleContEnv\n",
    "\n",
    "    ENV_NAME = 'CartPole-v0'\n",
    "    gym.undo_logger_setup()\n",
    "\n",
    "    # Get the environment and extract the number of actions.\n",
    "    env = gym.make(ENV_NAME)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    env.seed(123)\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    # Next, we build a very simple model.\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions))\n",
    "\n",
    "    # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "    # even the metrics!\n",
    "    memory = SequentialMemory(limit=60000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "          target_model_update=1e-2, policy=policy)\n",
    "    dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "    # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "    # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "    # Ctrl + C.\n",
    "    dqn.fit(env, nb_steps=60000, visualize=False, verbose=2)\n",
    "\n",
    "    # get model weights\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # Finally, evaluate our algorithm for 5 episodes.\n",
    "    dqn.test(env, nb_episodes=5, visualize=True)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_to_txt(weights, f):\n",
    "    shape = weights.shape\n",
    "    n_rows = shape[0]\n",
    "    n_cols = shape[1]\n",
    "    weights = np.reshape((n_cols, n_rows)) # do this in a prettier way w/transpose\n",
    "    for r in range(0,n_cols):\n",
    "            for c in range(0,n_rows - 1):\n",
    "                    f.write(str(weights[r,c])+\", \")\n",
    "            f.write(str(weights[r,c])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_to_txt(bias, f):\n",
    "    for i in range(0,len(bias)):\n",
    "        f.write(str(bias[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write cartpole nnet to text so that its format is the same as that of small_nnet\n",
    "'''\n",
    "def cartpole_nnet_to_txt():\n",
    "    layers = train_cartpole_nnet()\n",
    "    f = open(\"cartpole_nnet.txt\", \"w+\")\n",
    "    f.write(\"3\\n\")\n",
    "    f.write(\"4, 16, 16, 16, 2\\n\")\n",
    "    for i in range(5):\n",
    "        f.write(\"0\\n\")\n",
    "    for i,layer in enumerate(layers):\n",
    "        if i % 2 == 0:\n",
    "            weights_to_txt(layer, f)\n",
    "        else:\n",
    "            bias_to_txt(layer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amelia/anaconda3/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Training for 60000 steps ...\n",
      "    31/60000: episode: 1, duration: 0.096s, episode steps: 31, steps per second: 322, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.015 [-1.187, 1.822], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    54/60000: episode: 2, duration: 0.020s, episode steps: 23, steps per second: 1144, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.099 [-0.940, 1.811], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    79/60000: episode: 3, duration: 0.023s, episode steps: 25, steps per second: 1069, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.069 [-0.545, 1.124], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    99/60000: episode: 4, duration: 0.018s, episode steps: 20, steps per second: 1088, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.080 [-0.823, 1.643], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   110/60000: episode: 5, duration: 0.618s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.102 [-1.257, 0.819], loss: 0.482998, mean_absolute_error: 0.539642, mean_q: 0.110688\n",
      "   137/60000: episode: 6, duration: 0.133s, episode steps: 27, steps per second: 204, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.094 [-0.796, 1.709], loss: 0.332989, mean_absolute_error: 0.540275, mean_q: 0.296458\n",
      "   157/60000: episode: 7, duration: 0.090s, episode steps: 20, steps per second: 222, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.073 [-1.137, 1.999], loss: 0.154448, mean_absolute_error: 0.572417, mean_q: 0.686609\n",
      "   175/60000: episode: 8, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.086 [-0.988, 1.645], loss: 0.084110, mean_absolute_error: 0.656068, mean_q: 1.056217\n",
      "   219/60000: episode: 9, duration: 0.236s, episode steps: 44, steps per second: 186, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: 0.050 [-1.135, 2.028], loss: 0.050528, mean_absolute_error: 0.747524, mean_q: 1.383657\n",
      "   228/60000: episode: 10, duration: 0.039s, episode steps: 9, steps per second: 231, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.117 [-1.194, 1.954], loss: 0.046419, mean_absolute_error: 0.853129, mean_q: 1.628797\n",
      "   238/60000: episode: 11, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.205, 1.936], loss: 0.035684, mean_absolute_error: 0.851986, mean_q: 1.656115\n",
      "   249/60000: episode: 12, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.104 [-1.487, 1.017], loss: 0.035557, mean_absolute_error: 0.899026, mean_q: 1.798500\n",
      "   266/60000: episode: 13, duration: 0.112s, episode steps: 17, steps per second: 151, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.087 [-1.451, 0.650], loss: 0.047219, mean_absolute_error: 0.944340, mean_q: 1.857644\n",
      "   280/60000: episode: 14, duration: 0.065s, episode steps: 14, steps per second: 214, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.086 [-1.389, 0.806], loss: 0.033938, mean_absolute_error: 1.007995, mean_q: 2.015009\n",
      "   289/60000: episode: 15, duration: 0.038s, episode steps: 9, steps per second: 234, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.116 [-1.182, 1.907], loss: 0.047831, mean_absolute_error: 1.042674, mean_q: 2.064741\n",
      "   302/60000: episode: 16, duration: 0.059s, episode steps: 13, steps per second: 222, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.097 [-0.783, 1.434], loss: 0.049496, mean_absolute_error: 1.100851, mean_q: 2.166355\n",
      "   321/60000: episode: 17, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.046 [-1.544, 2.354], loss: 0.054444, mean_absolute_error: 1.158339, mean_q: 2.286072\n",
      "   344/60000: episode: 18, duration: 0.098s, episode steps: 23, steps per second: 234, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.065 [-1.002, 1.847], loss: 0.074564, mean_absolute_error: 1.265937, mean_q: 2.476648\n",
      "   364/60000: episode: 19, duration: 0.089s, episode steps: 20, steps per second: 224, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.084 [-1.928, 3.030], loss: 0.058681, mean_absolute_error: 1.354647, mean_q: 2.713126\n",
      "   378/60000: episode: 20, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.111 [-0.791, 1.555], loss: 0.102314, mean_absolute_error: 1.443210, mean_q: 2.733232\n",
      "   388/60000: episode: 21, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.144 [-0.963, 1.712], loss: 0.068883, mean_absolute_error: 1.520682, mean_q: 3.009478\n",
      "   414/60000: episode: 22, duration: 0.111s, episode steps: 26, steps per second: 234, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.600, 1.205], loss: 0.113800, mean_absolute_error: 1.554408, mean_q: 2.986098\n",
      "   446/60000: episode: 23, duration: 0.177s, episode steps: 32, steps per second: 180, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.068 [-0.950, 1.757], loss: 0.099332, mean_absolute_error: 1.678708, mean_q: 3.274561\n",
      "   465/60000: episode: 24, duration: 0.089s, episode steps: 19, steps per second: 213, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.082 [-1.360, 2.277], loss: 0.120516, mean_absolute_error: 1.775931, mean_q: 3.463712\n",
      "   485/60000: episode: 25, duration: 0.089s, episode steps: 20, steps per second: 225, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.090 [-2.038, 1.166], loss: 0.141076, mean_absolute_error: 1.867102, mean_q: 3.561822\n",
      "   504/60000: episode: 26, duration: 0.119s, episode steps: 19, steps per second: 159, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.070 [-2.252, 1.390], loss: 0.117940, mean_absolute_error: 1.939770, mean_q: 3.742865\n",
      "   530/60000: episode: 27, duration: 0.108s, episode steps: 26, steps per second: 240, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.048 [-0.789, 1.290], loss: 0.117477, mean_absolute_error: 2.033391, mean_q: 3.932751\n",
      "   543/60000: episode: 28, duration: 0.061s, episode steps: 13, steps per second: 212, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.087 [-0.834, 1.338], loss: 0.131172, mean_absolute_error: 2.102762, mean_q: 4.013352\n",
      "   569/60000: episode: 29, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.014 [-1.950, 1.407], loss: 0.122538, mean_absolute_error: 2.186450, mean_q: 4.203212\n",
      "   588/60000: episode: 30, duration: 0.082s, episode steps: 19, steps per second: 231, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.054 [-2.094, 1.411], loss: 0.167610, mean_absolute_error: 2.245773, mean_q: 4.275941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   626/60000: episode: 31, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.158 [-1.354, 0.870], loss: 0.141783, mean_absolute_error: 2.356678, mean_q: 4.465603\n",
      "   661/60000: episode: 32, duration: 0.159s, episode steps: 35, steps per second: 220, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.004 [-1.208, 1.627], loss: 0.182793, mean_absolute_error: 2.472494, mean_q: 4.678588\n",
      "   679/60000: episode: 33, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.074 [-1.769, 1.011], loss: 0.083976, mean_absolute_error: 2.596426, mean_q: 5.038244\n",
      "   722/60000: episode: 34, duration: 0.190s, episode steps: 43, steps per second: 226, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.082 [-1.056, 0.634], loss: 0.149260, mean_absolute_error: 2.681924, mean_q: 5.137877\n",
      "   762/60000: episode: 35, duration: 0.243s, episode steps: 40, steps per second: 164, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.073 [-1.380, 0.561], loss: 0.159163, mean_absolute_error: 2.831513, mean_q: 5.450237\n",
      "   777/60000: episode: 36, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.081 [-1.007, 1.715], loss: 0.172082, mean_absolute_error: 2.922625, mean_q: 5.626667\n",
      "   806/60000: episode: 37, duration: 0.116s, episode steps: 29, steps per second: 249, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.076 [-1.474, 0.627], loss: 0.172376, mean_absolute_error: 3.015946, mean_q: 5.805136\n",
      "   836/60000: episode: 38, duration: 0.142s, episode steps: 30, steps per second: 211, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.044 [-1.080, 0.797], loss: 0.158018, mean_absolute_error: 3.129081, mean_q: 6.074984\n",
      "   853/60000: episode: 39, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.102 [-1.400, 0.604], loss: 0.158611, mean_absolute_error: 3.224996, mean_q: 6.319915\n",
      "   894/60000: episode: 40, duration: 0.188s, episode steps: 41, steps per second: 219, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.048 [-1.350, 1.431], loss: 0.194318, mean_absolute_error: 3.328757, mean_q: 6.466237\n",
      "   918/60000: episode: 41, duration: 0.105s, episode steps: 24, steps per second: 229, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.042 [-1.298, 0.791], loss: 0.153810, mean_absolute_error: 3.478084, mean_q: 6.793275\n",
      "  1018/60000: episode: 42, duration: 0.461s, episode steps: 100, steps per second: 217, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.093 [-1.011, 1.625], loss: 0.165001, mean_absolute_error: 3.711748, mean_q: 7.351661\n",
      "  1072/60000: episode: 43, duration: 0.275s, episode steps: 54, steps per second: 196, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.895, 0.990], loss: 0.148247, mean_absolute_error: 4.038851, mean_q: 8.029934\n",
      "  1114/60000: episode: 44, duration: 0.180s, episode steps: 42, steps per second: 234, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.100 [-0.934, 1.412], loss: 0.201166, mean_absolute_error: 4.206811, mean_q: 8.326459\n",
      "  1157/60000: episode: 45, duration: 0.206s, episode steps: 43, steps per second: 209, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.080 [-0.555, 1.313], loss: 0.159590, mean_absolute_error: 4.396994, mean_q: 8.744306\n",
      "  1171/60000: episode: 46, duration: 0.063s, episode steps: 14, steps per second: 223, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.121 [-0.944, 1.590], loss: 0.142821, mean_absolute_error: 4.564483, mean_q: 9.115859\n",
      "  1250/60000: episode: 47, duration: 0.362s, episode steps: 79, steps per second: 218, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.054 [-1.475, 1.310], loss: 0.209930, mean_absolute_error: 4.718152, mean_q: 9.384910\n",
      "  1278/60000: episode: 48, duration: 0.118s, episode steps: 28, steps per second: 237, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.043 [-1.642, 1.132], loss: 0.245205, mean_absolute_error: 4.921829, mean_q: 9.793170\n",
      "  1368/60000: episode: 49, duration: 0.403s, episode steps: 90, steps per second: 223, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.204 [-1.467, 1.145], loss: 0.266044, mean_absolute_error: 5.130778, mean_q: 10.215357\n",
      "  1457/60000: episode: 50, duration: 0.393s, episode steps: 89, steps per second: 226, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.296 [-1.855, 1.133], loss: 0.271191, mean_absolute_error: 5.471166, mean_q: 10.951962\n",
      "  1474/60000: episode: 51, duration: 0.075s, episode steps: 17, steps per second: 226, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.079 [-1.300, 0.820], loss: 0.271367, mean_absolute_error: 5.662780, mean_q: 11.441777\n",
      "  1527/60000: episode: 52, duration: 0.247s, episode steps: 53, steps per second: 214, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.007 [-1.386, 1.120], loss: 0.358720, mean_absolute_error: 5.840129, mean_q: 11.724266\n",
      "  1665/60000: episode: 53, duration: 0.595s, episode steps: 138, steps per second: 232, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.265 [-1.063, 1.795], loss: 0.368640, mean_absolute_error: 6.278604, mean_q: 12.649339\n",
      "  1752/60000: episode: 54, duration: 0.393s, episode steps: 87, steps per second: 221, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.209 [-1.487, 0.776], loss: 0.444999, mean_absolute_error: 6.725559, mean_q: 13.607496\n",
      "  1881/60000: episode: 55, duration: 0.586s, episode steps: 129, steps per second: 220, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.220 [-1.417, 1.539], loss: 0.474928, mean_absolute_error: 7.213273, mean_q: 14.590459\n",
      "  2010/60000: episode: 56, duration: 0.576s, episode steps: 129, steps per second: 224, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.113 [-1.259, 1.117], loss: 0.458620, mean_absolute_error: 7.731936, mean_q: 15.759458\n",
      "  2210/60000: episode: 57, duration: 0.858s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.126 [-1.873, 1.143], loss: 0.642134, mean_absolute_error: 8.547544, mean_q: 17.400068\n",
      "  2381/60000: episode: 58, duration: 0.749s, episode steps: 171, steps per second: 228, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.333 [-2.346, 0.951], loss: 0.837109, mean_absolute_error: 9.360050, mean_q: 19.079565\n",
      "  2581/60000: episode: 59, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-1.191, 1.402], loss: 0.827832, mean_absolute_error: 10.266702, mean_q: 20.963160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2768/60000: episode: 60, duration: 0.801s, episode steps: 187, steps per second: 233, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.348 [-2.416, 1.255], loss: 0.952939, mean_absolute_error: 11.206593, mean_q: 22.859228\n",
      "  2968/60000: episode: 61, duration: 0.857s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.271 [-1.909, 1.013], loss: 1.110746, mean_absolute_error: 12.104518, mean_q: 24.673775\n",
      "  3168/60000: episode: 62, duration: 0.842s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.042 [-1.278, 1.197], loss: 1.375261, mean_absolute_error: 13.061201, mean_q: 26.563278\n",
      "  3368/60000: episode: 63, duration: 0.871s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-1.153, 1.003], loss: 1.601450, mean_absolute_error: 13.981603, mean_q: 28.436455\n",
      "  3545/60000: episode: 64, duration: 0.750s, episode steps: 177, steps per second: 236, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.388 [-2.323, 1.026], loss: 1.980397, mean_absolute_error: 14.835007, mean_q: 30.115383\n",
      "  3736/60000: episode: 65, duration: 0.833s, episode steps: 191, steps per second: 229, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.353 [-2.435, 1.032], loss: 1.805543, mean_absolute_error: 15.575244, mean_q: 31.696054\n",
      "  3916/60000: episode: 66, duration: 0.790s, episode steps: 180, steps per second: 228, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.374 [-2.408, 1.058], loss: 1.846651, mean_absolute_error: 16.321001, mean_q: 33.301006\n",
      "  4116/60000: episode: 67, duration: 0.869s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.335 [-2.299, 0.850], loss: 1.612166, mean_absolute_error: 17.193529, mean_q: 35.016647\n",
      "  4316/60000: episode: 68, duration: 0.879s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.335 [-2.322, 0.965], loss: 2.134818, mean_absolute_error: 17.928913, mean_q: 36.446098\n",
      "  4516/60000: episode: 69, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.266 [-1.830, 0.880], loss: 2.804921, mean_absolute_error: 18.715660, mean_q: 37.998444\n",
      "  4716/60000: episode: 70, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.311 [-2.235, 0.978], loss: 3.066766, mean_absolute_error: 19.556879, mean_q: 39.610756\n",
      "  4916/60000: episode: 71, duration: 0.854s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.186 [-1.340, 0.874], loss: 2.295681, mean_absolute_error: 20.252052, mean_q: 41.138626\n",
      "  5116/60000: episode: 72, duration: 0.863s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.110 [-1.103, 0.975], loss: 3.194482, mean_absolute_error: 21.119108, mean_q: 42.742012\n",
      "  5296/60000: episode: 73, duration: 0.790s, episode steps: 180, steps per second: 228, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.384 [-2.422, 0.838], loss: 2.935463, mean_absolute_error: 21.668005, mean_q: 43.964954\n",
      "  5496/60000: episode: 74, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.102 [-1.459, 0.909], loss: 3.106900, mean_absolute_error: 22.470678, mean_q: 45.652111\n",
      "  5696/60000: episode: 75, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.254 [-1.891, 1.008], loss: 4.451303, mean_absolute_error: 23.003616, mean_q: 46.593746\n",
      "  5896/60000: episode: 76, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.158 [-1.260, 1.149], loss: 3.319294, mean_absolute_error: 23.748657, mean_q: 48.194550\n",
      "  6073/60000: episode: 77, duration: 0.750s, episode steps: 177, steps per second: 236, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.372 [-2.410, 1.031], loss: 3.916159, mean_absolute_error: 24.364202, mean_q: 49.437412\n",
      "  6273/60000: episode: 78, duration: 0.874s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.131 [-1.264, 1.296], loss: 2.812233, mean_absolute_error: 25.016777, mean_q: 50.681553\n",
      "  6473/60000: episode: 79, duration: 0.869s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.228 [-1.585, 1.135], loss: 2.621898, mean_absolute_error: 25.607500, mean_q: 51.946480\n",
      "  6651/60000: episode: 80, duration: 0.767s, episode steps: 178, steps per second: 232, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.384 [-2.412, 0.775], loss: 4.718507, mean_absolute_error: 26.280191, mean_q: 53.125221\n",
      "  6847/60000: episode: 81, duration: 0.842s, episode steps: 196, steps per second: 233, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.348 [-2.419, 0.878], loss: 3.720492, mean_absolute_error: 26.723513, mean_q: 54.083714\n",
      "  7028/60000: episode: 82, duration: 0.773s, episode steps: 181, steps per second: 234, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.377 [-2.428, 0.779], loss: 4.422012, mean_absolute_error: 27.233377, mean_q: 55.051601\n",
      "  7228/60000: episode: 83, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.248 [-1.673, 1.163], loss: 4.430625, mean_absolute_error: 27.575718, mean_q: 55.818302\n",
      "  7428/60000: episode: 84, duration: 0.869s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.254 [-1.809, 1.189], loss: 3.620413, mean_absolute_error: 28.247377, mean_q: 57.308983\n",
      "  7628/60000: episode: 85, duration: 0.865s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.119 [-1.263, 1.229], loss: 3.102244, mean_absolute_error: 28.757614, mean_q: 58.260498\n",
      "  7828/60000: episode: 86, duration: 0.861s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.172 [-1.475, 0.897], loss: 3.243814, mean_absolute_error: 29.391411, mean_q: 59.435455\n",
      "  8028/60000: episode: 87, duration: 0.851s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.242 [-1.692, 1.120], loss: 3.822799, mean_absolute_error: 29.801113, mean_q: 60.295650\n",
      "  8228/60000: episode: 88, duration: 0.860s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.228 [-1.709, 0.909], loss: 5.288465, mean_absolute_error: 30.161743, mean_q: 60.865326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8428/60000: episode: 89, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.150 [-1.342, 1.043], loss: 4.155929, mean_absolute_error: 30.800024, mean_q: 62.279736\n",
      "  8628/60000: episode: 90, duration: 0.846s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.292 [-2.035, 1.067], loss: 4.065501, mean_absolute_error: 31.096691, mean_q: 62.856640\n",
      "  8828/60000: episode: 91, duration: 0.874s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.143 [-1.258, 1.074], loss: 4.669473, mean_absolute_error: 31.580479, mean_q: 63.903099\n",
      "  9028/60000: episode: 92, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.103 [-1.166, 1.093], loss: 2.942264, mean_absolute_error: 31.903442, mean_q: 64.601852\n",
      "  9228/60000: episode: 93, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.222 [-1.569, 1.105], loss: 5.003870, mean_absolute_error: 32.373505, mean_q: 65.382225\n",
      "  9428/60000: episode: 94, duration: 1.085s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.261 [-1.859, 0.951], loss: 3.874331, mean_absolute_error: 32.751110, mean_q: 66.293839\n",
      "  9628/60000: episode: 95, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.295 [-1.070, 2.591], loss: 3.595521, mean_absolute_error: 33.276417, mean_q: 67.259537\n",
      "  9828/60000: episode: 96, duration: 1.102s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.224 [-1.113, 1.870], loss: 3.816164, mean_absolute_error: 33.581146, mean_q: 67.957558\n",
      " 10028/60000: episode: 97, duration: 0.971s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.133 [-1.514, 1.159], loss: 6.823088, mean_absolute_error: 34.140575, mean_q: 69.002296\n",
      " 10228/60000: episode: 98, duration: 1.121s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.031 [-1.102, 1.138], loss: 4.694666, mean_absolute_error: 34.368320, mean_q: 69.425171\n",
      " 10428/60000: episode: 99, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.046 [-1.111, 1.280], loss: 3.824754, mean_absolute_error: 34.809837, mean_q: 70.385979\n",
      " 10628/60000: episode: 100, duration: 1.542s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.284 [-2.020, 0.940], loss: 5.051881, mean_absolute_error: 35.495007, mean_q: 71.743896\n",
      " 10828/60000: episode: 101, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.131 [-1.308, 1.034], loss: 5.558027, mean_absolute_error: 35.190781, mean_q: 71.017670\n",
      " 11028/60000: episode: 102, duration: 1.551s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.199 [-1.406, 1.081], loss: 6.094594, mean_absolute_error: 35.753788, mean_q: 72.183067\n",
      " 11228/60000: episode: 103, duration: 2.171s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.200 [-1.659, 1.176], loss: 8.096563, mean_absolute_error: 35.934292, mean_q: 72.368416\n",
      " 11428/60000: episode: 104, duration: 1.633s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.175 [-1.292, 1.307], loss: 4.328706, mean_absolute_error: 35.851105, mean_q: 72.351669\n",
      " 11628/60000: episode: 105, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.312 [-2.299, 1.212], loss: 4.717216, mean_absolute_error: 36.396168, mean_q: 73.371506\n",
      " 11828/60000: episode: 106, duration: 1.078s, episode steps: 200, steps per second: 186, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.201 [-1.480, 1.252], loss: 5.438808, mean_absolute_error: 36.476105, mean_q: 73.571457\n",
      " 12028/60000: episode: 107, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.263 [-1.165, 2.197], loss: 5.877999, mean_absolute_error: 36.646225, mean_q: 73.949066\n",
      " 12182/60000: episode: 108, duration: 1.070s, episode steps: 154, steps per second: 144, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.384 [-0.843, 2.327], loss: 3.165812, mean_absolute_error: 36.726814, mean_q: 74.167397\n",
      " 12338/60000: episode: 109, duration: 1.267s, episode steps: 156, steps per second: 123, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.393 [-1.155, 2.410], loss: 5.786189, mean_absolute_error: 37.017429, mean_q: 74.710426\n",
      " 12538/60000: episode: 110, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.077 [-1.258, 1.383], loss: 5.745034, mean_absolute_error: 37.340328, mean_q: 75.338768\n",
      " 12738/60000: episode: 111, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.159 [-1.253, 1.455], loss: 6.248302, mean_absolute_error: 37.658237, mean_q: 75.991730\n",
      " 12897/60000: episode: 112, duration: 0.943s, episode steps: 159, steps per second: 169, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.371 [-1.162, 2.308], loss: 9.994597, mean_absolute_error: 37.678780, mean_q: 75.965744\n",
      " 13097/60000: episode: 113, duration: 1.187s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-1.448, 1.216], loss: 3.012965, mean_absolute_error: 38.242977, mean_q: 77.274994\n",
      " 13281/60000: episode: 114, duration: 1.178s, episode steps: 184, steps per second: 156, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.343 [-2.440, 1.068], loss: 4.215620, mean_absolute_error: 38.394104, mean_q: 77.616653\n",
      " 13481/60000: episode: 115, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.259 [-1.818, 1.163], loss: 8.322582, mean_absolute_error: 38.210251, mean_q: 77.095909\n",
      " 13647/60000: episode: 116, duration: 0.789s, episode steps: 166, steps per second: 210, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.365 [-1.140, 2.326], loss: 8.331779, mean_absolute_error: 38.583561, mean_q: 77.662521\n",
      " 13847/60000: episode: 117, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.097 [-1.493, 1.081], loss: 4.093668, mean_absolute_error: 38.576660, mean_q: 77.912758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14047/60000: episode: 118, duration: 0.874s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.200 [-1.714, 1.526], loss: 7.375447, mean_absolute_error: 38.528652, mean_q: 77.552094\n",
      " 14247/60000: episode: 119, duration: 0.857s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.153 [-1.675, 1.592], loss: 4.230103, mean_absolute_error: 38.705849, mean_q: 78.191727\n",
      " 14447/60000: episode: 120, duration: 0.859s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.283 [-1.198, 2.228], loss: 5.166572, mean_absolute_error: 38.793324, mean_q: 78.362175\n",
      " 14616/60000: episode: 121, duration: 0.729s, episode steps: 169, steps per second: 232, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.373 [-2.420, 1.438], loss: 7.344231, mean_absolute_error: 38.681236, mean_q: 77.819016\n",
      " 14767/60000: episode: 122, duration: 0.672s, episode steps: 151, steps per second: 225, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.380 [-0.875, 2.231], loss: 8.287043, mean_absolute_error: 39.162239, mean_q: 78.859337\n",
      " 14967/60000: episode: 123, duration: 0.868s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.249 [-2.252, 1.128], loss: 6.590359, mean_absolute_error: 38.746319, mean_q: 78.003441\n",
      " 15167/60000: episode: 124, duration: 0.868s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.285 [-1.265, 2.206], loss: 3.207111, mean_absolute_error: 38.930927, mean_q: 78.687782\n",
      " 15367/60000: episode: 125, duration: 0.859s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.060 [-1.041, 1.110], loss: 5.439708, mean_absolute_error: 39.042912, mean_q: 78.847382\n",
      " 15567/60000: episode: 126, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.294 [-2.562, 1.086], loss: 5.525164, mean_absolute_error: 39.235645, mean_q: 79.302254\n",
      " 15739/60000: episode: 127, duration: 0.751s, episode steps: 172, steps per second: 229, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.333 [-1.268, 2.225], loss: 6.037764, mean_absolute_error: 39.193398, mean_q: 79.263481\n",
      " 15875/60000: episode: 128, duration: 0.591s, episode steps: 136, steps per second: 230, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.400 [-1.225, 2.117], loss: 2.232186, mean_absolute_error: 39.540627, mean_q: 79.929688\n",
      " 16073/60000: episode: 129, duration: 0.866s, episode steps: 198, steps per second: 229, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.277 [-1.084, 2.106], loss: 8.933750, mean_absolute_error: 39.476170, mean_q: 79.675400\n",
      " 16230/60000: episode: 130, duration: 0.699s, episode steps: 157, steps per second: 225, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.327 [-0.893, 1.988], loss: 6.064556, mean_absolute_error: 39.207348, mean_q: 79.264259\n",
      " 16430/60000: episode: 131, duration: 0.865s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.248 [-1.738, 1.305], loss: 5.259299, mean_absolute_error: 39.387699, mean_q: 79.499641\n",
      " 16630/60000: episode: 132, duration: 0.864s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.029 [-1.151, 1.263], loss: 4.723619, mean_absolute_error: 39.369339, mean_q: 79.568398\n",
      " 16776/60000: episode: 133, duration: 0.640s, episode steps: 146, steps per second: 228, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.375 [-0.941, 2.130], loss: 9.139897, mean_absolute_error: 40.049328, mean_q: 80.563606\n",
      " 16976/60000: episode: 134, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.140 [-1.489, 1.471], loss: 5.705010, mean_absolute_error: 39.472729, mean_q: 79.586624\n",
      " 17146/60000: episode: 135, duration: 1.492s, episode steps: 170, steps per second: 114, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.326 [-1.261, 2.133], loss: 4.193727, mean_absolute_error: 40.096668, mean_q: 80.949356\n",
      " 17346/60000: episode: 136, duration: 2.019s, episode steps: 200, steps per second: 99, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.230 [-1.210, 1.969], loss: 5.863776, mean_absolute_error: 39.715950, mean_q: 80.142860\n",
      " 17546/60000: episode: 137, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.243 [-1.834, 1.324], loss: 7.093783, mean_absolute_error: 39.449020, mean_q: 79.540794\n",
      " 17691/60000: episode: 138, duration: 1.087s, episode steps: 145, steps per second: 133, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.366 [-1.054, 2.053], loss: 4.158296, mean_absolute_error: 39.760075, mean_q: 80.114075\n",
      " 17891/60000: episode: 139, duration: 1.108s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.109 [-1.276, 1.203], loss: 5.557158, mean_absolute_error: 39.708412, mean_q: 80.188522\n",
      " 18069/60000: episode: 140, duration: 0.929s, episode steps: 178, steps per second: 192, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.401 [-2.404, 1.621], loss: 2.921624, mean_absolute_error: 39.984627, mean_q: 80.666824\n",
      " 18256/60000: episode: 141, duration: 1.080s, episode steps: 187, steps per second: 173, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.287 [-1.015, 2.076], loss: 7.161497, mean_absolute_error: 39.937595, mean_q: 80.563942\n",
      " 18427/60000: episode: 142, duration: 0.814s, episode steps: 171, steps per second: 210, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.389 [-2.404, 1.054], loss: 4.309484, mean_absolute_error: 39.945316, mean_q: 80.719559\n",
      " 18558/60000: episode: 143, duration: 0.850s, episode steps: 131, steps per second: 154, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.415 [-0.778, 2.128], loss: 5.133776, mean_absolute_error: 40.004990, mean_q: 80.739647\n",
      " 18721/60000: episode: 144, duration: 0.774s, episode steps: 163, steps per second: 211, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.353 [-0.903, 2.191], loss: 5.288630, mean_absolute_error: 39.801025, mean_q: 80.425377\n",
      " 18907/60000: episode: 145, duration: 1.012s, episode steps: 186, steps per second: 184, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.310 [-0.899, 2.233], loss: 5.285982, mean_absolute_error: 39.457813, mean_q: 79.692749\n",
      " 19107/60000: episode: 146, duration: 0.836s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.200 [-1.508, 0.961], loss: 4.165087, mean_absolute_error: 40.019360, mean_q: 80.969238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19307/60000: episode: 147, duration: 0.845s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.197 [-1.534, 1.238], loss: 8.354635, mean_absolute_error: 39.751591, mean_q: 80.115555\n",
      " 19454/60000: episode: 148, duration: 0.623s, episode steps: 147, steps per second: 236, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.334 [-0.896, 2.022], loss: 5.478581, mean_absolute_error: 40.205608, mean_q: 81.094505\n",
      " 19641/60000: episode: 149, duration: 0.801s, episode steps: 187, steps per second: 234, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.317 [-1.489, 2.380], loss: 5.708466, mean_absolute_error: 39.826935, mean_q: 80.330154\n",
      " 19841/60000: episode: 150, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.315 [-2.120, 1.534], loss: 4.746910, mean_absolute_error: 39.824894, mean_q: 80.279343\n",
      " 19994/60000: episode: 151, duration: 0.660s, episode steps: 153, steps per second: 232, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.347 [-0.914, 2.028], loss: 5.714530, mean_absolute_error: 39.670631, mean_q: 80.059021\n",
      " 20194/60000: episode: 152, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.088 [-1.183, 1.121], loss: 2.350392, mean_absolute_error: 39.898434, mean_q: 80.657288\n",
      " 20370/60000: episode: 153, duration: 0.802s, episode steps: 176, steps per second: 220, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.557 [0.000, 1.000], mean observation: 0.339 [-2.231, 3.746], loss: 3.811772, mean_absolute_error: 39.908279, mean_q: 80.568146\n",
      " 20522/60000: episode: 154, duration: 0.693s, episode steps: 152, steps per second: 219, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.390 [-1.519, 3.317], loss: 9.223077, mean_absolute_error: 40.033054, mean_q: 80.721733\n",
      " 20683/60000: episode: 155, duration: 0.723s, episode steps: 161, steps per second: 223, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.368 [-1.910, 3.553], loss: 3.005785, mean_absolute_error: 39.823891, mean_q: 80.484497\n",
      " 20868/60000: episode: 156, duration: 1.301s, episode steps: 185, steps per second: 142, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.557 [0.000, 1.000], mean observation: 0.307 [-2.806, 3.926], loss: 7.950755, mean_absolute_error: 40.141575, mean_q: 80.858444\n",
      " 21042/60000: episode: 157, duration: 0.900s, episode steps: 174, steps per second: 193, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.557 [0.000, 1.000], mean observation: 0.334 [-2.227, 3.718], loss: 3.488045, mean_absolute_error: 39.933865, mean_q: 80.514008\n",
      " 21226/60000: episode: 158, duration: 0.805s, episode steps: 184, steps per second: 229, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.321 [-1.676, 3.357], loss: 4.416122, mean_absolute_error: 39.556995, mean_q: 79.815353\n",
      " 21390/60000: episode: 159, duration: 0.708s, episode steps: 164, steps per second: 232, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.381 [-0.910, 2.421], loss: 2.264542, mean_absolute_error: 39.841290, mean_q: 80.497879\n",
      " 21590/60000: episode: 160, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.105 [-1.278, 1.483], loss: 4.307944, mean_absolute_error: 39.888283, mean_q: 80.565643\n",
      " 21750/60000: episode: 161, duration: 1.120s, episode steps: 160, steps per second: 143, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.368 [-2.051, 3.722], loss: 3.097857, mean_absolute_error: 39.782619, mean_q: 80.367821\n",
      " 21889/60000: episode: 162, duration: 1.209s, episode steps: 139, steps per second: 115, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.445 [-0.917, 2.434], loss: 4.694470, mean_absolute_error: 39.981293, mean_q: 80.773987\n",
      " 22089/60000: episode: 163, duration: 2.667s, episode steps: 200, steps per second: 75, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.228 [-1.188, 2.163], loss: 3.326756, mean_absolute_error: 39.838741, mean_q: 80.595886\n",
      " 22228/60000: episode: 164, duration: 1.480s, episode steps: 139, steps per second: 94, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.427 [-1.182, 3.169], loss: 5.688113, mean_absolute_error: 39.994125, mean_q: 80.776711\n",
      " 22428/60000: episode: 165, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.284 [-1.159, 2.936], loss: 5.873342, mean_absolute_error: 39.675350, mean_q: 80.085159\n",
      " 22595/60000: episode: 166, duration: 1.509s, episode steps: 167, steps per second: 111, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.557 [0.000, 1.000], mean observation: 0.359 [-1.751, 3.486], loss: 4.011299, mean_absolute_error: 40.123497, mean_q: 81.010902\n",
      " 22740/60000: episode: 167, duration: 1.534s, episode steps: 145, steps per second: 95, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.425 [-0.698, 2.758], loss: 5.240675, mean_absolute_error: 39.815403, mean_q: 80.384453\n",
      " 22940/60000: episode: 168, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.167 [-1.133, 1.693], loss: 2.773302, mean_absolute_error: 39.928246, mean_q: 80.767929\n",
      " 23130/60000: episode: 169, duration: 1.218s, episode steps: 190, steps per second: 156, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.319 [-1.638, 3.339], loss: 3.233304, mean_absolute_error: 40.420258, mean_q: 81.546364\n",
      " 23306/60000: episode: 170, duration: 1.196s, episode steps: 176, steps per second: 147, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.354 [-1.169, 2.964], loss: 4.277459, mean_absolute_error: 40.221779, mean_q: 81.077950\n",
      " 23506/60000: episode: 171, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.018 [-1.283, 1.487], loss: 4.552117, mean_absolute_error: 40.399139, mean_q: 81.590897\n",
      " 23674/60000: episode: 172, duration: 1.035s, episode steps: 168, steps per second: 162, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.353 [-1.844, 3.395], loss: 3.696147, mean_absolute_error: 40.042511, mean_q: 80.840401\n",
      " 23861/60000: episode: 173, duration: 1.094s, episode steps: 187, steps per second: 171, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.333 [-0.921, 2.804], loss: 2.761215, mean_absolute_error: 40.242004, mean_q: 81.272316\n",
      " 24056/60000: episode: 174, duration: 0.932s, episode steps: 195, steps per second: 209, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.305 [-1.801, 3.486], loss: 2.835747, mean_absolute_error: 40.674850, mean_q: 82.216805\n",
      " 24219/60000: episode: 175, duration: 0.956s, episode steps: 163, steps per second: 171, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.375 [-0.824, 2.788], loss: 4.048700, mean_absolute_error: 40.122662, mean_q: 81.042130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24419/60000: episode: 176, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.063 [-1.179, 1.358], loss: 5.261411, mean_absolute_error: 40.349491, mean_q: 81.347595\n",
      " 24616/60000: episode: 177, duration: 1.057s, episode steps: 197, steps per second: 186, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.319 [-1.005, 2.774], loss: 3.008866, mean_absolute_error: 40.700863, mean_q: 82.152710\n",
      " 24804/60000: episode: 178, duration: 1.243s, episode steps: 188, steps per second: 151, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.328 [-0.832, 2.356], loss: 5.785369, mean_absolute_error: 40.415112, mean_q: 81.433304\n",
      " 24951/60000: episode: 179, duration: 0.692s, episode steps: 147, steps per second: 212, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.425 [-0.556, 2.406], loss: 5.669284, mean_absolute_error: 40.508373, mean_q: 81.662666\n",
      " 25116/60000: episode: 180, duration: 0.833s, episode steps: 165, steps per second: 198, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.353 [-1.033, 2.228], loss: 3.654500, mean_absolute_error: 40.498726, mean_q: 81.772766\n",
      " 25316/60000: episode: 181, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.207 [-1.090, 1.854], loss: 5.536119, mean_absolute_error: 40.364452, mean_q: 81.424835\n",
      " 25513/60000: episode: 182, duration: 1.498s, episode steps: 197, steps per second: 132, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.290 [-0.969, 2.137], loss: 7.399201, mean_absolute_error: 40.616051, mean_q: 81.923096\n",
      " 25697/60000: episode: 183, duration: 1.351s, episode steps: 184, steps per second: 136, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.347 [-0.989, 2.398], loss: 4.752229, mean_absolute_error: 40.382515, mean_q: 81.634987\n",
      " 25885/60000: episode: 184, duration: 0.941s, episode steps: 188, steps per second: 200, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.336 [-1.107, 2.417], loss: 3.806307, mean_absolute_error: 40.114471, mean_q: 81.089821\n",
      " 26048/60000: episode: 185, duration: 0.807s, episode steps: 163, steps per second: 202, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.381 [-0.826, 2.773], loss: 5.101388, mean_absolute_error: 40.567684, mean_q: 82.034836\n",
      " 26248/60000: episode: 186, duration: 1.001s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.066 [-1.187, 1.411], loss: 5.704148, mean_absolute_error: 40.479568, mean_q: 81.733192\n",
      " 26420/60000: episode: 187, duration: 0.763s, episode steps: 172, steps per second: 225, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.328 [-0.940, 2.112], loss: 4.580728, mean_absolute_error: 40.395546, mean_q: 81.656494\n",
      " 26620/60000: episode: 188, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.214 [-1.192, 1.797], loss: 5.181039, mean_absolute_error: 41.085102, mean_q: 83.053894\n",
      " 26783/60000: episode: 189, duration: 0.764s, episode steps: 163, steps per second: 213, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.361 [-0.994, 2.240], loss: 4.334853, mean_absolute_error: 41.158016, mean_q: 83.224411\n",
      " 26983/60000: episode: 190, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.289 [-1.040, 2.173], loss: 3.856442, mean_absolute_error: 40.796909, mean_q: 82.581078\n",
      " 27150/60000: episode: 191, duration: 1.152s, episode steps: 167, steps per second: 145, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.369 [-0.672, 2.349], loss: 3.514006, mean_absolute_error: 40.825138, mean_q: 82.632721\n",
      " 27347/60000: episode: 192, duration: 0.877s, episode steps: 197, steps per second: 225, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.306 [-1.016, 2.256], loss: 3.821472, mean_absolute_error: 40.956646, mean_q: 82.978622\n",
      " 27517/60000: episode: 193, duration: 0.861s, episode steps: 170, steps per second: 197, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.361 [-0.796, 2.322], loss: 5.522204, mean_absolute_error: 41.310638, mean_q: 83.525169\n",
      " 27689/60000: episode: 194, duration: 1.390s, episode steps: 172, steps per second: 124, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.339 [-1.003, 2.212], loss: 1.533354, mean_absolute_error: 41.393440, mean_q: 83.894119\n",
      " 27858/60000: episode: 195, duration: 0.754s, episode steps: 169, steps per second: 224, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.340 [-1.206, 2.156], loss: 3.357015, mean_absolute_error: 41.278591, mean_q: 83.417793\n",
      " 28010/60000: episode: 196, duration: 1.349s, episode steps: 152, steps per second: 113, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.375 [-0.697, 2.147], loss: 4.304214, mean_absolute_error: 41.571205, mean_q: 83.872108\n",
      " 28166/60000: episode: 197, duration: 0.743s, episode steps: 156, steps per second: 210, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.382 [-1.009, 2.305], loss: 3.307443, mean_absolute_error: 40.922440, mean_q: 82.809380\n",
      " 28326/60000: episode: 198, duration: 0.920s, episode steps: 160, steps per second: 174, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.367 [-0.866, 2.228], loss: 2.724349, mean_absolute_error: 41.566547, mean_q: 84.180222\n",
      " 28475/60000: episode: 199, duration: 0.875s, episode steps: 149, steps per second: 170, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.375 [-0.770, 2.140], loss: 4.527483, mean_absolute_error: 41.963345, mean_q: 84.973511\n",
      " 28638/60000: episode: 200, duration: 1.025s, episode steps: 163, steps per second: 159, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.364 [-0.885, 2.335], loss: 3.890079, mean_absolute_error: 41.908672, mean_q: 84.744598\n",
      " 28788/60000: episode: 201, duration: 0.888s, episode steps: 150, steps per second: 169, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.373 [-0.756, 2.131], loss: 1.498327, mean_absolute_error: 41.473312, mean_q: 83.924133\n",
      " 28963/60000: episode: 202, duration: 1.006s, episode steps: 175, steps per second: 174, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.364 [-0.950, 2.520], loss: 3.907416, mean_absolute_error: 41.378769, mean_q: 83.707954\n",
      " 29134/60000: episode: 203, duration: 0.973s, episode steps: 171, steps per second: 176, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.335 [-0.683, 2.204], loss: 3.236467, mean_absolute_error: 41.591541, mean_q: 84.094757\n",
      " 29296/60000: episode: 204, duration: 1.075s, episode steps: 162, steps per second: 151, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.354 [-0.567, 2.173], loss: 4.652122, mean_absolute_error: 41.588242, mean_q: 84.068085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29464/60000: episode: 205, duration: 1.003s, episode steps: 168, steps per second: 167, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.358 [-0.868, 2.283], loss: 3.420611, mean_absolute_error: 41.967278, mean_q: 84.785561\n",
      " 29633/60000: episode: 206, duration: 1.244s, episode steps: 169, steps per second: 136, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.363 [-0.767, 2.286], loss: 4.884663, mean_absolute_error: 41.643204, mean_q: 84.280647\n",
      " 29820/60000: episode: 207, duration: 0.996s, episode steps: 187, steps per second: 188, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.327 [-0.992, 2.282], loss: 3.025326, mean_absolute_error: 41.732155, mean_q: 84.553238\n",
      " 29996/60000: episode: 208, duration: 0.965s, episode steps: 176, steps per second: 182, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.362 [-0.786, 2.386], loss: 1.960970, mean_absolute_error: 41.739513, mean_q: 84.671967\n",
      " 30169/60000: episode: 209, duration: 1.557s, episode steps: 173, steps per second: 111, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.339 [-0.757, 2.180], loss: 2.653299, mean_absolute_error: 41.835209, mean_q: 84.735909\n",
      " 30342/60000: episode: 210, duration: 1.623s, episode steps: 173, steps per second: 107, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.359 [-0.713, 2.314], loss: 4.100555, mean_absolute_error: 42.215374, mean_q: 85.489120\n",
      " 30526/60000: episode: 211, duration: 2.051s, episode steps: 184, steps per second: 90, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.327 [-0.697, 2.217], loss: 4.904397, mean_absolute_error: 42.174000, mean_q: 85.276848\n",
      " 30708/60000: episode: 212, duration: 1.557s, episode steps: 182, steps per second: 117, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.316 [-0.757, 2.119], loss: 4.535591, mean_absolute_error: 42.353916, mean_q: 85.725449\n",
      " 30897/60000: episode: 213, duration: 1.860s, episode steps: 189, steps per second: 102, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.320 [-0.687, 2.211], loss: 2.574529, mean_absolute_error: 42.685196, mean_q: 86.558250\n",
      " 31089/60000: episode: 214, duration: 1.612s, episode steps: 192, steps per second: 119, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.329 [-0.653, 2.404], loss: 4.372171, mean_absolute_error: 42.248627, mean_q: 85.495583\n",
      " 31286/60000: episode: 215, duration: 0.961s, episode steps: 197, steps per second: 205, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.299 [-0.611, 2.119], loss: 3.696930, mean_absolute_error: 42.959431, mean_q: 87.018829\n",
      " 31483/60000: episode: 216, duration: 0.968s, episode steps: 197, steps per second: 204, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.310 [-0.594, 2.245], loss: 3.796731, mean_absolute_error: 43.020638, mean_q: 87.131027\n",
      " 31682/60000: episode: 217, duration: 1.506s, episode steps: 199, steps per second: 132, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.327 [-0.698, 2.372], loss: 3.763710, mean_absolute_error: 42.786884, mean_q: 86.650948\n",
      " 31882/60000: episode: 218, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.224 [-0.893, 1.784], loss: 3.387064, mean_absolute_error: 43.007603, mean_q: 87.090134\n",
      " 32082/60000: episode: 219, duration: 1.021s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.233 [-0.754, 1.838], loss: 4.573634, mean_absolute_error: 43.250957, mean_q: 87.574768\n",
      " 32282/60000: episode: 220, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.306 [-0.828, 2.407], loss: 5.828769, mean_absolute_error: 43.363819, mean_q: 87.882843\n",
      " 32478/60000: episode: 221, duration: 1.298s, episode steps: 196, steps per second: 151, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.273 [-0.795, 2.146], loss: 4.213138, mean_absolute_error: 43.811111, mean_q: 88.761436\n",
      " 32678/60000: episode: 222, duration: 1.160s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.258 [-0.839, 1.874], loss: 3.869671, mean_absolute_error: 43.556114, mean_q: 88.253883\n",
      " 32872/60000: episode: 223, duration: 1.049s, episode steps: 194, steps per second: 185, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.290 [-0.685, 2.011], loss: 5.674483, mean_absolute_error: 44.108803, mean_q: 89.383286\n",
      " 33072/60000: episode: 224, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.235 [-0.618, 2.005], loss: 4.207609, mean_absolute_error: 44.093861, mean_q: 89.345398\n",
      " 33272/60000: episode: 225, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.210 [-0.919, 1.712], loss: 3.620532, mean_absolute_error: 44.160038, mean_q: 89.549355\n",
      " 33462/60000: episode: 226, duration: 0.827s, episode steps: 190, steps per second: 230, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.335 [-0.882, 2.373], loss: 3.712990, mean_absolute_error: 44.279308, mean_q: 89.620766\n",
      " 33662/60000: episode: 227, duration: 0.993s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.270 [-1.197, 2.809], loss: 2.949317, mean_absolute_error: 44.647072, mean_q: 90.402382\n",
      " 33856/60000: episode: 228, duration: 0.859s, episode steps: 194, steps per second: 226, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.298 [-0.676, 2.179], loss: 4.498271, mean_absolute_error: 44.172115, mean_q: 89.357330\n",
      " 34056/60000: episode: 229, duration: 1.065s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.204 [-0.870, 1.679], loss: 5.150194, mean_absolute_error: 45.007812, mean_q: 90.912521\n",
      " 34256/60000: episode: 230, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.162 [-0.817, 1.630], loss: 4.200339, mean_absolute_error: 44.670547, mean_q: 90.429688\n",
      " 34456/60000: episode: 231, duration: 1.112s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.142 [-0.828, 1.423], loss: 3.989063, mean_absolute_error: 44.709419, mean_q: 90.591194\n",
      " 34647/60000: episode: 232, duration: 1.064s, episode steps: 191, steps per second: 179, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.303 [-0.695, 2.414], loss: 9.723333, mean_absolute_error: 44.843903, mean_q: 90.539085\n",
      " 34847/60000: episode: 233, duration: 0.978s, episode steps: 200, steps per second: 204, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.105 [-0.935, 1.260], loss: 2.130532, mean_absolute_error: 45.057442, mean_q: 91.214203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35043/60000: episode: 234, duration: 0.896s, episode steps: 196, steps per second: 219, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.280 [-0.909, 2.029], loss: 4.045689, mean_absolute_error: 45.252659, mean_q: 91.561195\n",
      " 35243/60000: episode: 235, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.101 [-0.948, 1.321], loss: 4.366701, mean_absolute_error: 44.964787, mean_q: 90.948341\n",
      " 35443/60000: episode: 236, duration: 0.860s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.113 [-0.942, 1.064], loss: 4.367002, mean_absolute_error: 44.945084, mean_q: 91.079857\n",
      " 35643/60000: episode: 237, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.160 [-0.844, 1.322], loss: 5.924421, mean_absolute_error: 44.885365, mean_q: 90.760857\n",
      " 35843/60000: episode: 238, duration: 0.837s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-0.943, 0.776], loss: 6.317202, mean_absolute_error: 44.901955, mean_q: 90.815842\n",
      " 36043/60000: episode: 239, duration: 0.863s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.919, 1.011], loss: 3.689235, mean_absolute_error: 45.382900, mean_q: 92.023895\n",
      " 36243/60000: episode: 240, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.953, 0.802], loss: 3.065942, mean_absolute_error: 45.598515, mean_q: 92.298271\n",
      " 36443/60000: episode: 241, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-0.820, 0.584], loss: 9.287028, mean_absolute_error: 44.521137, mean_q: 90.180466\n",
      " 36643/60000: episode: 242, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.990, 0.724], loss: 2.935946, mean_absolute_error: 45.093300, mean_q: 91.505882\n",
      " 36843/60000: episode: 243, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.019 [-1.025, 0.815], loss: 5.796168, mean_absolute_error: 45.631336, mean_q: 92.354324\n",
      " 37043/60000: episode: 244, duration: 0.840s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.792, 0.900], loss: 3.706009, mean_absolute_error: 46.052052, mean_q: 93.329681\n",
      " 37243/60000: episode: 245, duration: 0.875s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-0.791, 0.730], loss: 4.368844, mean_absolute_error: 45.704601, mean_q: 92.572853\n",
      " 37443/60000: episode: 246, duration: 0.830s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-0.814, 0.645], loss: 2.410757, mean_absolute_error: 46.618507, mean_q: 94.444046\n",
      " 37643/60000: episode: 247, duration: 0.868s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.009 [-0.868, 0.813], loss: 3.337095, mean_absolute_error: 46.234894, mean_q: 93.722038\n",
      " 37843/60000: episode: 248, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-1.064, 0.735], loss: 5.105389, mean_absolute_error: 46.787266, mean_q: 94.703728\n",
      " 38043/60000: episode: 249, duration: 0.842s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-0.797, 0.817], loss: 5.967093, mean_absolute_error: 46.791504, mean_q: 94.821663\n",
      " 38243/60000: episode: 250, duration: 0.859s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.931, 0.740], loss: 3.663856, mean_absolute_error: 46.885441, mean_q: 95.033539\n",
      " 38443/60000: episode: 251, duration: 0.828s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-1.106, 0.975], loss: 12.262604, mean_absolute_error: 46.985130, mean_q: 94.855705\n",
      " 38643/60000: episode: 252, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.010 [-0.909, 0.861], loss: 6.745227, mean_absolute_error: 47.663773, mean_q: 96.432579\n",
      " 38843/60000: episode: 253, duration: 0.843s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.028 [-1.142, 1.207], loss: 9.792913, mean_absolute_error: 47.517384, mean_q: 95.858780\n",
      " 39043/60000: episode: 254, duration: 0.840s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-1.074, 0.859], loss: 8.733913, mean_absolute_error: 47.384884, mean_q: 95.770584\n",
      " 39243/60000: episode: 255, duration: 0.826s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-1.065, 0.892], loss: 14.276411, mean_absolute_error: 48.035137, mean_q: 96.757332\n",
      " 39443/60000: episode: 256, duration: 0.840s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-1.104, 0.816], loss: 5.549736, mean_absolute_error: 47.841015, mean_q: 96.737617\n",
      " 39643/60000: episode: 257, duration: 0.854s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-1.082, 0.814], loss: 3.107695, mean_absolute_error: 47.475330, mean_q: 96.250984\n",
      " 39843/60000: episode: 258, duration: 0.853s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.019 [-0.972, 0.805], loss: 7.077957, mean_absolute_error: 48.436710, mean_q: 97.989433\n",
      " 40043/60000: episode: 259, duration: 0.845s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-0.999, 0.957], loss: 8.651318, mean_absolute_error: 47.914059, mean_q: 97.073875\n",
      " 40243/60000: episode: 260, duration: 0.848s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-1.029, 1.089], loss: 6.602435, mean_absolute_error: 48.412121, mean_q: 97.850838\n",
      " 40443/60000: episode: 261, duration: 0.835s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.103, 1.072], loss: 12.146972, mean_absolute_error: 48.339931, mean_q: 97.653038\n",
      " 40643/60000: episode: 262, duration: 0.842s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.025 [-0.950, 1.151], loss: 15.214635, mean_absolute_error: 48.818848, mean_q: 98.521545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40843/60000: episode: 263, duration: 0.858s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.028 [-1.120, 1.050], loss: 13.727439, mean_absolute_error: 48.556984, mean_q: 98.146965\n",
      " 41043/60000: episode: 264, duration: 0.853s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.022 [-1.113, 0.823], loss: 8.249223, mean_absolute_error: 49.431446, mean_q: 99.936668\n",
      " 41243/60000: episode: 265, duration: 0.865s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-1.122, 1.289], loss: 8.964634, mean_absolute_error: 48.968945, mean_q: 99.051735\n",
      " 41443/60000: episode: 266, duration: 0.848s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.015 [-1.096, 1.093], loss: 12.504517, mean_absolute_error: 49.015541, mean_q: 99.081856\n",
      " 41643/60000: episode: 267, duration: 0.855s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.016 [-1.316, 1.079], loss: 9.595486, mean_absolute_error: 49.441036, mean_q: 99.826561\n",
      " 41843/60000: episode: 268, duration: 0.853s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.023 [-1.081, 0.912], loss: 14.828379, mean_absolute_error: 50.047287, mean_q: 100.815941\n",
      " 42043/60000: episode: 269, duration: 0.848s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-1.060, 0.905], loss: 8.605526, mean_absolute_error: 49.806133, mean_q: 100.582199\n",
      " 42243/60000: episode: 270, duration: 0.862s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-1.125, 1.043], loss: 11.825544, mean_absolute_error: 50.137440, mean_q: 101.054726\n",
      " 42443/60000: episode: 271, duration: 0.865s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.030 [-1.220, 0.963], loss: 16.783785, mean_absolute_error: 50.023575, mean_q: 100.849022\n",
      " 42643/60000: episode: 272, duration: 0.849s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.026 [-1.118, 1.059], loss: 13.759656, mean_absolute_error: 49.933662, mean_q: 100.771629\n",
      " 42843/60000: episode: 273, duration: 0.862s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.071, 0.906], loss: 10.709919, mean_absolute_error: 50.551773, mean_q: 102.036644\n",
      " 43043/60000: episode: 274, duration: 0.866s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-0.944, 0.817], loss: 13.901772, mean_absolute_error: 50.485332, mean_q: 101.848083\n",
      " 43243/60000: episode: 275, duration: 0.851s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.024 [-1.079, 0.805], loss: 13.913844, mean_absolute_error: 50.522598, mean_q: 102.133278\n",
      " 43443/60000: episode: 276, duration: 0.858s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.030 [-0.982, 0.883], loss: 8.551289, mean_absolute_error: 51.165161, mean_q: 103.254318\n",
      " 43643/60000: episode: 277, duration: 0.860s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-1.001, 1.191], loss: 10.475840, mean_absolute_error: 51.019791, mean_q: 103.000549\n",
      " 43843/60000: episode: 278, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.037, 0.901], loss: 11.431982, mean_absolute_error: 51.162106, mean_q: 103.208748\n",
      " 44043/60000: episode: 279, duration: 0.856s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.033 [-1.068, 1.014], loss: 9.257824, mean_absolute_error: 51.751934, mean_q: 104.506737\n",
      " 44243/60000: episode: 280, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.034 [-0.858, 0.759], loss: 12.890481, mean_absolute_error: 51.320236, mean_q: 103.531006\n",
      " 44443/60000: episode: 281, duration: 0.853s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.035 [-0.971, 0.901], loss: 12.269053, mean_absolute_error: 51.710060, mean_q: 104.189804\n",
      " 44643/60000: episode: 282, duration: 0.855s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.038 [-1.029, 0.861], loss: 6.844420, mean_absolute_error: 51.476318, mean_q: 103.832100\n",
      " 44843/60000: episode: 283, duration: 0.838s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-0.931, 1.049], loss: 8.021112, mean_absolute_error: 51.512100, mean_q: 103.825676\n",
      " 45043/60000: episode: 284, duration: 0.853s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.037 [-0.969, 0.890], loss: 11.491190, mean_absolute_error: 52.683002, mean_q: 106.073799\n",
      " 45243/60000: episode: 285, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.037 [-0.971, 1.239], loss: 19.770567, mean_absolute_error: 52.570751, mean_q: 105.529961\n",
      " 45443/60000: episode: 286, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.037 [-1.141, 1.108], loss: 17.340157, mean_absolute_error: 52.589058, mean_q: 105.778885\n",
      " 45643/60000: episode: 287, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.035 [-1.127, 1.082], loss: 10.522017, mean_absolute_error: 52.733376, mean_q: 106.233040\n",
      " 45843/60000: episode: 288, duration: 0.854s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-1.058, 1.067], loss: 14.411106, mean_absolute_error: 52.958752, mean_q: 106.369980\n",
      " 46043/60000: episode: 289, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.028 [-0.970, 0.947], loss: 10.728194, mean_absolute_error: 53.109310, mean_q: 106.821640\n",
      " 46243/60000: episode: 290, duration: 0.853s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.025 [-1.083, 0.930], loss: 17.652927, mean_absolute_error: 53.022980, mean_q: 106.686249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46443/60000: episode: 291, duration: 0.857s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.024 [-0.909, 0.941], loss: 17.694050, mean_absolute_error: 53.332245, mean_q: 107.383575\n",
      " 46643/60000: episode: 292, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-1.004, 0.804], loss: 9.503999, mean_absolute_error: 53.446495, mean_q: 107.792648\n",
      " 46843/60000: episode: 293, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-1.039, 0.849], loss: 9.999101, mean_absolute_error: 53.676289, mean_q: 108.291580\n",
      " 47043/60000: episode: 294, duration: 1.175s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.226, 1.072], loss: 19.684637, mean_absolute_error: 54.388878, mean_q: 109.331535\n",
      " 47243/60000: episode: 295, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.021 [-1.063, 0.848], loss: 18.576336, mean_absolute_error: 54.405010, mean_q: 109.489647\n",
      " 47443/60000: episode: 296, duration: 1.146s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.025 [-1.359, 1.156], loss: 10.862530, mean_absolute_error: 54.912350, mean_q: 110.683968\n",
      " 47643/60000: episode: 297, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-1.303, 1.153], loss: 19.653065, mean_absolute_error: 54.769165, mean_q: 110.027977\n",
      " 47843/60000: episode: 298, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-1.103, 0.936], loss: 21.623529, mean_absolute_error: 54.947300, mean_q: 110.331673\n",
      " 48043/60000: episode: 299, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.023 [-1.108, 1.133], loss: 21.262815, mean_absolute_error: 55.252491, mean_q: 110.906296\n",
      " 48243/60000: episode: 300, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.009 [-1.086, 0.900], loss: 24.052359, mean_absolute_error: 55.259609, mean_q: 110.984062\n",
      " 48443/60000: episode: 301, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.022 [-0.989, 0.810], loss: 13.801120, mean_absolute_error: 55.892914, mean_q: 112.477654\n",
      " 48643/60000: episode: 302, duration: 0.905s, episode steps: 200, steps per second: 221, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-1.178, 1.201], loss: 9.741006, mean_absolute_error: 55.837208, mean_q: 112.491463\n",
      " 48843/60000: episode: 303, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.021 [-0.864, 0.874], loss: 19.631798, mean_absolute_error: 55.854813, mean_q: 112.345108\n",
      " 49043/60000: episode: 304, duration: 0.863s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.949, 1.135], loss: 14.606337, mean_absolute_error: 56.059166, mean_q: 112.824532\n",
      " 49243/60000: episode: 305, duration: 0.868s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.037 [-1.176, 1.180], loss: 18.621616, mean_absolute_error: 55.823475, mean_q: 112.262794\n",
      " 49443/60000: episode: 306, duration: 0.840s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-1.212, 1.188], loss: 10.727988, mean_absolute_error: 56.091007, mean_q: 112.968430\n",
      " 49643/60000: episode: 307, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-1.237, 1.038], loss: 17.377600, mean_absolute_error: 55.759766, mean_q: 112.072662\n",
      " 49843/60000: episode: 308, duration: 0.849s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.019 [-1.369, 1.448], loss: 16.005440, mean_absolute_error: 56.100998, mean_q: 112.501282\n",
      " 50043/60000: episode: 309, duration: 0.849s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.018 [-1.388, 1.374], loss: 14.442710, mean_absolute_error: 56.227211, mean_q: 112.820091\n",
      " 50243/60000: episode: 310, duration: 0.865s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.027 [-0.982, 1.083], loss: 14.371874, mean_absolute_error: 55.595722, mean_q: 111.477921\n",
      " 50443/60000: episode: 311, duration: 0.871s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.147, 1.158], loss: 16.948591, mean_absolute_error: 55.789833, mean_q: 111.840782\n",
      " 50643/60000: episode: 312, duration: 0.852s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-1.136, 1.162], loss: 15.238012, mean_absolute_error: 55.400414, mean_q: 110.967697\n",
      " 50843/60000: episode: 313, duration: 0.870s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.009 [-1.446, 1.271], loss: 17.897141, mean_absolute_error: 55.791069, mean_q: 111.729431\n",
      " 51043/60000: episode: 314, duration: 0.836s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-1.340, 1.186], loss: 13.967367, mean_absolute_error: 55.815590, mean_q: 111.907692\n",
      " 51243/60000: episode: 315, duration: 0.866s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-1.300, 1.258], loss: 13.724327, mean_absolute_error: 55.048008, mean_q: 110.350937\n",
      " 51443/60000: episode: 316, duration: 0.862s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.018 [-1.301, 1.554], loss: 16.003927, mean_absolute_error: 55.012447, mean_q: 110.200272\n",
      " 51643/60000: episode: 317, duration: 0.863s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.007 [-1.509, 1.335], loss: 16.392023, mean_absolute_error: 54.083664, mean_q: 108.240440\n",
      " 51843/60000: episode: 318, duration: 0.857s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-1.489, 1.285], loss: 10.879995, mean_absolute_error: 54.460068, mean_q: 109.275436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52043/60000: episode: 319, duration: 0.865s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.065 [-1.136, 1.222], loss: 21.559299, mean_absolute_error: 54.314842, mean_q: 108.723679\n",
      " 52243/60000: episode: 320, duration: 0.858s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-1.306, 1.338], loss: 25.911543, mean_absolute_error: 54.347725, mean_q: 108.699356\n",
      " 52443/60000: episode: 321, duration: 0.851s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.085 [-1.261, 1.184], loss: 20.164495, mean_absolute_error: 53.578178, mean_q: 107.162483\n",
      " 52643/60000: episode: 322, duration: 0.870s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.090 [-1.237, 1.534], loss: 16.991745, mean_absolute_error: 52.942402, mean_q: 105.928024\n",
      " 52843/60000: episode: 323, duration: 0.853s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.336 [-1.036, 2.390], loss: 15.118896, mean_absolute_error: 52.794434, mean_q: 105.862236\n",
      " 53019/60000: episode: 324, duration: 0.887s, episode steps: 176, steps per second: 198, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.352 [-0.982, 2.295], loss: 11.893108, mean_absolute_error: 52.178333, mean_q: 104.755371\n",
      " 53219/60000: episode: 325, duration: 1.064s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.033 [-1.063, 1.309], loss: 18.282167, mean_absolute_error: 51.774357, mean_q: 103.884003\n",
      " 53419/60000: episode: 326, duration: 2.532s, episode steps: 200, steps per second: 79, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-1.107, 1.115], loss: 13.932392, mean_absolute_error: 51.423019, mean_q: 103.187141\n",
      " 53619/60000: episode: 327, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.093 [-1.119, 0.943], loss: 14.313647, mean_absolute_error: 51.391041, mean_q: 103.349998\n",
      " 53819/60000: episode: 328, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.081 [-1.191, 1.141], loss: 18.106577, mean_absolute_error: 51.098164, mean_q: 102.583160\n",
      " 54019/60000: episode: 329, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.008 [-1.038, 1.141], loss: 22.091568, mean_absolute_error: 50.875259, mean_q: 101.850723\n",
      " 54219/60000: episode: 330, duration: 1.069s, episode steps: 200, steps per second: 187, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.047 [-1.320, 1.287], loss: 24.536755, mean_absolute_error: 50.115467, mean_q: 100.020409\n",
      " 54419/60000: episode: 331, duration: 1.031s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-1.320, 1.472], loss: 15.211554, mean_absolute_error: 49.848465, mean_q: 99.979904\n",
      " 54619/60000: episode: 332, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.073 [-1.027, 1.482], loss: 13.235139, mean_absolute_error: 50.205997, mean_q: 100.548103\n",
      " 54819/60000: episode: 333, duration: 0.875s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.060 [-1.408, 1.329], loss: 15.671435, mean_absolute_error: 49.517159, mean_q: 99.136154\n",
      " 55019/60000: episode: 334, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-1.315, 1.117], loss: 20.861160, mean_absolute_error: 49.702724, mean_q: 99.289658\n",
      " 55219/60000: episode: 335, duration: 0.862s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.083 [-0.945, 0.957], loss: 5.531568, mean_absolute_error: 48.877674, mean_q: 98.126991\n",
      " 55412/60000: episode: 336, duration: 0.825s, episode steps: 193, steps per second: 234, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.357 [-1.337, 2.435], loss: 10.623004, mean_absolute_error: 49.080727, mean_q: 98.582878\n",
      " 55612/60000: episode: 337, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.067 [-1.366, 1.337], loss: 12.607038, mean_absolute_error: 48.851879, mean_q: 97.928673\n",
      " 55812/60000: episode: 338, duration: 0.874s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.067 [-1.366, 1.383], loss: 20.185223, mean_absolute_error: 48.763916, mean_q: 97.454521\n",
      " 56012/60000: episode: 339, duration: 0.837s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.078 [-1.131, 1.439], loss: 15.508391, mean_absolute_error: 48.777451, mean_q: 97.776535\n",
      " 56212/60000: episode: 340, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.151 [-1.105, 1.368], loss: 16.128380, mean_absolute_error: 48.266266, mean_q: 96.693596\n",
      " 56412/60000: episode: 341, duration: 0.875s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.108 [-1.310, 1.343], loss: 19.313484, mean_absolute_error: 48.167530, mean_q: 96.504456\n",
      " 56612/60000: episode: 342, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.229 [-1.412, 1.672], loss: 24.131914, mean_absolute_error: 48.640869, mean_q: 97.046837\n",
      " 56800/60000: episode: 343, duration: 0.825s, episode steps: 188, steps per second: 228, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.347 [-1.339, 2.416], loss: 10.887605, mean_absolute_error: 48.449421, mean_q: 97.302399\n",
      " 57000/60000: episode: 344, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.083 [-1.414, 1.300], loss: 12.074837, mean_absolute_error: 48.365654, mean_q: 97.073486\n",
      " 57200/60000: episode: 345, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.052 [-1.400, 1.570], loss: 16.063335, mean_absolute_error: 48.795563, mean_q: 97.732330\n",
      " 57400/60000: episode: 346, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.057 [-1.202, 1.469], loss: 11.206159, mean_absolute_error: 48.484180, mean_q: 97.244598\n",
      " 57600/60000: episode: 347, duration: 0.853s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.030 [-1.378, 1.343], loss: 15.288327, mean_absolute_error: 48.355293, mean_q: 96.811485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57800/60000: episode: 348, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-1.486, 1.456], loss: 19.606665, mean_absolute_error: 47.922676, mean_q: 95.683655\n",
      " 58000/60000: episode: 349, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.020 [-1.324, 1.186], loss: 9.468140, mean_absolute_error: 48.072128, mean_q: 96.391983\n",
      " 58200/60000: episode: 350, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-1.382, 1.339], loss: 11.498200, mean_absolute_error: 47.957520, mean_q: 96.162148\n",
      " 58400/60000: episode: 351, duration: 0.863s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.015 [-1.464, 1.732], loss: 13.528465, mean_absolute_error: 47.710537, mean_q: 95.540245\n",
      " 58600/60000: episode: 352, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.151, 1.268], loss: 9.459664, mean_absolute_error: 47.660858, mean_q: 95.705910\n",
      " 58800/60000: episode: 353, duration: 0.958s, episode steps: 200, steps per second: 209, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.005 [-1.433, 1.068], loss: 15.360171, mean_absolute_error: 48.011452, mean_q: 96.297264\n",
      " 59000/60000: episode: 354, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.008 [-1.297, 1.520], loss: 11.547283, mean_absolute_error: 47.709198, mean_q: 95.777985\n",
      " 59200/60000: episode: 355, duration: 0.869s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-1.162, 1.219], loss: 8.907758, mean_absolute_error: 47.693676, mean_q: 95.803185\n",
      " 59400/60000: episode: 356, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.009 [-1.230, 1.285], loss: 7.956038, mean_absolute_error: 47.629654, mean_q: 95.616425\n",
      " 59600/60000: episode: 357, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-1.293, 1.386], loss: 11.225617, mean_absolute_error: 47.744579, mean_q: 95.730644\n",
      " 59797/60000: episode: 358, duration: 0.891s, episode steps: 197, steps per second: 221, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.333 [-0.955, 2.430], loss: 10.468990, mean_absolute_error: 47.752899, mean_q: 95.687271\n",
      " 59997/60000: episode: 359, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.036 [-1.456, 1.238], loss: 18.273317, mean_absolute_error: 47.402065, mean_q: 94.595238\n",
      "done, took 316.261 seconds\n",
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "abstract",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3490b432acdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartpole_nnet_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-0e565f0504fe>\u001b[0m in \u001b[0;36mcartpole_nnet_to_txt\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcartpole_nnet_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cartpole_nnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cartpole_nnet.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7608f1f389f5>\u001b[0m in \u001b[0;36mtrain_cartpole_nnet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'on_action_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0maxleoffset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, display)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_closed_by_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_default_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         '''\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_screens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         '''\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_default_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: abstract"
     ]
    }
   ],
   "source": [
    "weights = cartpole_nnet_to_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
