{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amelia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole_nnet():\n",
    "    #from test import CartPoleContEnv\n",
    "\n",
    "    ENV_NAME = 'CartPole-v0'\n",
    "    gym.undo_logger_setup()\n",
    "\n",
    "    # Get the environment and extract the number of actions.\n",
    "    env = gym.make(ENV_NAME)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    env.seed(123)\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    # Next, we build a very simple model.\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions))\n",
    "\n",
    "    # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "    # even the metrics!\n",
    "    memory = SequentialMemory(limit=60000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "          target_model_update=1e-2, policy=policy)\n",
    "    dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "    # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "    # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "    # Ctrl + C.\n",
    "    dqn.fit(env, nb_steps=60000, visualize=False, verbose=2)\n",
    "\n",
    "    # get model weights\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # Finally, evaluate our algorithm for 5 episodes.\n",
    "    dqn.test(env, nb_episodes=5, visualize=True)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_to_txt(weights, f):\n",
    "    shape = weights.shape\n",
    "    n_rows = shape[0]\n",
    "    n_cols = shape[1]\n",
    "    weights = np.reshape((n_cols, n_rows))\n",
    "    for r in range(0,n_cols):\n",
    "            for c in range(0,n_rows - 1):\n",
    "                    f.write(str(weights[r,c])+\", \")\n",
    "            f.write(str(weights[r,c])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_to_txt(bias, f):\n",
    "    for i in range(0,len(bias)):\n",
    "        f.write(str(bias[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write cartpole nnet to text so that its format is the same as that of small_nnet\n",
    "'''\n",
    "def cartpole_nnet_to_txt():\n",
    "    layers = train_cartpole_nnet()\n",
    "    f = open(\"cartpole_nnet.txt\", \"w+\")\n",
    "    f.write(\"4\\n\")\n",
    "    f.write(\"4, 16, 16, 16, 2\\n\")\n",
    "    for i in range(5):\n",
    "        f.write(\"0\\n\")\n",
    "    for i,layer in enumerate(layers):\n",
    "        if i % 2 == 0:\n",
    "            weights_to_txt(layer, f)\n",
    "        else:\n",
    "            bias_to_txt(layer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amelia/anaconda3/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Training for 60000 steps ...\n",
      "    31/60000: episode: 1, duration: 0.062s, episode steps: 31, steps per second: 502, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.015 [-1.187, 1.822], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    54/60000: episode: 2, duration: 0.018s, episode steps: 23, steps per second: 1282, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.099 [-0.940, 1.811], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    79/60000: episode: 3, duration: 0.020s, episode steps: 25, steps per second: 1234, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.069 [-0.545, 1.124], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    99/60000: episode: 4, duration: 0.017s, episode steps: 20, steps per second: 1167, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.080 [-0.823, 1.643], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   110/60000: episode: 5, duration: 0.554s, episode steps: 11, steps per second: 20, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.102 [-1.257, 0.819], loss: 0.478516, mean_absolute_error: 0.536664, mean_q: 0.114230\n",
      "   137/60000: episode: 6, duration: 0.123s, episode steps: 27, steps per second: 219, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.094 [-0.796, 1.709], loss: 0.337347, mean_absolute_error: 0.541251, mean_q: 0.292284\n",
      "   157/60000: episode: 7, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.073 [-1.137, 1.999], loss: 0.155389, mean_absolute_error: 0.569271, mean_q: 0.668405\n",
      "   175/60000: episode: 8, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.086 [-0.988, 1.645], loss: 0.085797, mean_absolute_error: 0.648230, mean_q: 1.045765\n",
      "   219/60000: episode: 9, duration: 0.404s, episode steps: 44, steps per second: 109, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: 0.050 [-1.135, 2.028], loss: 0.050742, mean_absolute_error: 0.741962, mean_q: 1.389498\n",
      "   228/60000: episode: 10, duration: 0.041s, episode steps: 9, steps per second: 218, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.117 [-1.194, 1.954], loss: 0.036369, mean_absolute_error: 0.810967, mean_q: 1.593719\n",
      "   238/60000: episode: 11, duration: 0.044s, episode steps: 10, steps per second: 227, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.205, 1.936], loss: 0.038596, mean_absolute_error: 0.857380, mean_q: 1.699719\n",
      "   249/60000: episode: 12, duration: 0.047s, episode steps: 11, steps per second: 234, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.104 [-1.487, 1.017], loss: 0.057160, mean_absolute_error: 0.910002, mean_q: 1.759492\n",
      "   272/60000: episode: 13, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.093 [-1.322, 0.631], loss: 0.053510, mean_absolute_error: 0.977707, mean_q: 1.881235\n",
      "   314/60000: episode: 14, duration: 0.173s, episode steps: 42, steps per second: 243, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.017 [-1.154, 1.797], loss: 0.052000, mean_absolute_error: 1.088912, mean_q: 2.104156\n",
      "   329/60000: episode: 15, duration: 0.102s, episode steps: 15, steps per second: 147, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.080 [-1.382, 2.315], loss: 0.052174, mean_absolute_error: 1.199424, mean_q: 2.323528\n",
      "   359/60000: episode: 16, duration: 0.133s, episode steps: 30, steps per second: 225, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.088 [-0.796, 1.729], loss: 0.076452, mean_absolute_error: 1.301645, mean_q: 2.512290\n",
      "   369/60000: episode: 17, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.937, 3.018], loss: 0.051410, mean_absolute_error: 1.355681, mean_q: 2.689406\n",
      "   385/60000: episode: 18, duration: 0.104s, episode steps: 16, steps per second: 153, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.085 [-1.383, 2.216], loss: 0.075351, mean_absolute_error: 1.428366, mean_q: 2.767045\n",
      "   408/60000: episode: 19, duration: 0.100s, episode steps: 23, steps per second: 230, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.072 [-1.707, 0.828], loss: 0.071170, mean_absolute_error: 1.523239, mean_q: 3.004611\n",
      "   423/60000: episode: 20, duration: 0.068s, episode steps: 15, steps per second: 221, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.096 [-0.787, 1.459], loss: 0.082834, mean_absolute_error: 1.593659, mean_q: 3.149466\n",
      "   441/60000: episode: 21, duration: 0.157s, episode steps: 18, steps per second: 115, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.121 [-0.779, 1.688], loss: 0.114886, mean_absolute_error: 1.674211, mean_q: 3.218495\n",
      "   461/60000: episode: 22, duration: 0.289s, episode steps: 20, steps per second: 69, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.071 [-1.526, 0.784], loss: 0.105404, mean_absolute_error: 1.756727, mean_q: 3.341981\n",
      "   472/60000: episode: 23, duration: 0.050s, episode steps: 11, steps per second: 218, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-1.142, 1.980], loss: 0.150958, mean_absolute_error: 1.804381, mean_q: 3.436552\n",
      "   488/60000: episode: 24, duration: 0.071s, episode steps: 16, steps per second: 226, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.064 [-2.479, 1.579], loss: 0.122911, mean_absolute_error: 1.869613, mean_q: 3.535077\n",
      "   509/60000: episode: 25, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.093 [-1.653, 0.968], loss: 0.120952, mean_absolute_error: 1.923499, mean_q: 3.670100\n",
      "   542/60000: episode: 26, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.084 [-0.592, 1.419], loss: 0.121949, mean_absolute_error: 2.031760, mean_q: 3.848362\n",
      "   564/60000: episode: 27, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.105 [-2.274, 1.180], loss: 0.143356, mean_absolute_error: 2.126992, mean_q: 4.058542\n",
      "   616/60000: episode: 28, duration: 0.253s, episode steps: 52, steps per second: 205, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.116 [-1.091, 2.144], loss: 0.138475, mean_absolute_error: 2.278124, mean_q: 4.314578\n",
      "   626/60000: episode: 29, duration: 0.046s, episode steps: 10, steps per second: 220, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.103 [-1.972, 1.225], loss: 0.103827, mean_absolute_error: 2.373167, mean_q: 4.518445\n",
      "   639/60000: episode: 30, duration: 0.052s, episode steps: 13, steps per second: 252, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.092 [-1.550, 1.023], loss: 0.140308, mean_absolute_error: 2.419974, mean_q: 4.586349\n",
      "   660/60000: episode: 31, duration: 0.092s, episode steps: 21, steps per second: 228, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.032 [-1.203, 1.795], loss: 0.148989, mean_absolute_error: 2.465404, mean_q: 4.662868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   677/60000: episode: 32, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.090 [-1.678, 0.941], loss: 0.129312, mean_absolute_error: 2.513361, mean_q: 4.780386\n",
      "   698/60000: episode: 33, duration: 0.090s, episode steps: 21, steps per second: 232, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.079 [-1.221, 0.805], loss: 0.123624, mean_absolute_error: 2.586859, mean_q: 4.962061\n",
      "   719/60000: episode: 34, duration: 0.094s, episode steps: 21, steps per second: 222, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.092 [-0.746, 1.574], loss: 0.138616, mean_absolute_error: 2.665992, mean_q: 5.126945\n",
      "   762/60000: episode: 35, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.102 [-1.380, 0.756], loss: 0.157979, mean_absolute_error: 2.763991, mean_q: 5.310550\n",
      "   812/60000: episode: 36, duration: 0.231s, episode steps: 50, steps per second: 217, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.058 [-0.605, 0.835], loss: 0.153880, mean_absolute_error: 2.965533, mean_q: 5.782404\n",
      "   835/60000: episode: 37, duration: 0.106s, episode steps: 23, steps per second: 216, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.098 [-1.058, 0.605], loss: 0.176836, mean_absolute_error: 3.098513, mean_q: 6.061229\n",
      "   894/60000: episode: 38, duration: 0.265s, episode steps: 59, steps per second: 223, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.160 [-1.494, 1.559], loss: 0.145872, mean_absolute_error: 3.271083, mean_q: 6.473519\n",
      "   922/60000: episode: 39, duration: 0.128s, episode steps: 28, steps per second: 219, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.130 [-0.914, 0.583], loss: 0.220634, mean_absolute_error: 3.461763, mean_q: 6.851693\n",
      "   954/60000: episode: 40, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.107, 0.583], loss: 0.226191, mean_absolute_error: 3.564976, mean_q: 7.053896\n",
      "   994/60000: episode: 41, duration: 0.167s, episode steps: 40, steps per second: 240, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.044 [-1.333, 1.156], loss: 0.195910, mean_absolute_error: 3.742682, mean_q: 7.422636\n",
      "  1025/60000: episode: 42, duration: 0.140s, episode steps: 31, steps per second: 222, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.057 [-1.018, 1.112], loss: 0.198393, mean_absolute_error: 3.888276, mean_q: 7.748974\n",
      "  1067/60000: episode: 43, duration: 0.182s, episode steps: 42, steps per second: 230, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-0.981, 0.620], loss: 0.230724, mean_absolute_error: 4.037791, mean_q: 8.066335\n",
      "  1098/60000: episode: 44, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.088 [-1.239, 0.697], loss: 0.209037, mean_absolute_error: 4.199120, mean_q: 8.395441\n",
      "  1131/60000: episode: 45, duration: 0.134s, episode steps: 33, steps per second: 246, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.062 [-0.564, 1.178], loss: 0.254483, mean_absolute_error: 4.327344, mean_q: 8.652159\n",
      "  1167/60000: episode: 46, duration: 0.165s, episode steps: 36, steps per second: 218, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.139 [-0.587, 1.775], loss: 0.276865, mean_absolute_error: 4.449616, mean_q: 8.888083\n",
      "  1367/60000: episode: 47, duration: 0.846s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.488, 1.618], loss: 0.261300, mean_absolute_error: 4.960639, mean_q: 9.936794\n",
      "  1460/60000: episode: 48, duration: 0.414s, episode steps: 93, steps per second: 225, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.244 [-1.610, 1.309], loss: 0.199924, mean_absolute_error: 5.598502, mean_q: 11.287926\n",
      "  1549/60000: episode: 49, duration: 0.391s, episode steps: 89, steps per second: 227, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.357 [-1.841, 1.142], loss: 0.339893, mean_absolute_error: 5.983309, mean_q: 12.081882\n",
      "  1743/60000: episode: 50, duration: 0.799s, episode steps: 194, steps per second: 243, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.207 [-1.683, 1.126], loss: 0.388550, mean_absolute_error: 6.597113, mean_q: 13.400685\n",
      "  1762/60000: episode: 51, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.069 [-1.491, 1.011], loss: 0.616976, mean_absolute_error: 7.097991, mean_q: 14.381955\n",
      "  1962/60000: episode: 52, duration: 0.849s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.170 [-1.302, 1.737], loss: 0.461929, mean_absolute_error: 7.570979, mean_q: 15.410605\n",
      "  2162/60000: episode: 53, duration: 0.848s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.060 [-1.357, 1.303], loss: 0.513595, mean_absolute_error: 8.482501, mean_q: 17.368382\n",
      "  2356/60000: episode: 54, duration: 0.818s, episode steps: 194, steps per second: 237, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.353 [-2.416, 0.905], loss: 0.791734, mean_absolute_error: 9.435074, mean_q: 19.300327\n",
      "  2556/60000: episode: 55, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.104 [-1.349, 1.176], loss: 0.797917, mean_absolute_error: 10.407925, mean_q: 21.309526\n",
      "  2726/60000: episode: 56, duration: 1.347s, episode steps: 170, steps per second: 126, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.379 [-2.407, 0.954], loss: 0.888288, mean_absolute_error: 11.374700, mean_q: 23.231812\n",
      "  2893/60000: episode: 57, duration: 0.939s, episode steps: 167, steps per second: 178, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.367 [-2.423, 0.961], loss: 1.252769, mean_absolute_error: 12.152426, mean_q: 24.805910\n",
      "  3093/60000: episode: 58, duration: 1.005s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.141 [-1.467, 1.108], loss: 1.644142, mean_absolute_error: 12.948472, mean_q: 26.407862\n",
      "  3293/60000: episode: 59, duration: 1.022s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.080 [-0.993, 0.853], loss: 1.499190, mean_absolute_error: 13.879176, mean_q: 28.409033\n",
      "  3442/60000: episode: 60, duration: 0.804s, episode steps: 149, steps per second: 185, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.255 [-1.252, 1.608], loss: 1.628994, mean_absolute_error: 14.639203, mean_q: 29.859627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3642/60000: episode: 61, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.017 [-1.258, 1.152], loss: 1.430736, mean_absolute_error: 15.365068, mean_q: 31.303343\n",
      "  3798/60000: episode: 62, duration: 1.100s, episode steps: 156, steps per second: 142, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.383 [-2.407, 0.970], loss: 1.993280, mean_absolute_error: 16.192730, mean_q: 32.923141\n",
      "  3978/60000: episode: 63, duration: 1.431s, episode steps: 180, steps per second: 126, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.355 [-2.420, 0.900], loss: 1.808962, mean_absolute_error: 16.795731, mean_q: 34.183064\n",
      "  4147/60000: episode: 64, duration: 1.172s, episode steps: 169, steps per second: 144, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.380 [-2.436, 0.786], loss: 2.704389, mean_absolute_error: 17.537285, mean_q: 35.611252\n",
      "  4313/60000: episode: 65, duration: 1.152s, episode steps: 166, steps per second: 144, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.391 [-2.427, 0.969], loss: 2.240659, mean_absolute_error: 18.090708, mean_q: 36.868286\n",
      "  4488/60000: episode: 66, duration: 1.325s, episode steps: 175, steps per second: 132, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.371 [-2.441, 0.901], loss: 2.635669, mean_absolute_error: 18.771809, mean_q: 38.077682\n",
      "  4641/60000: episode: 67, duration: 0.798s, episode steps: 153, steps per second: 192, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.411 [-2.418, 0.987], loss: 3.005131, mean_absolute_error: 19.282345, mean_q: 39.116863\n",
      "  4816/60000: episode: 68, duration: 0.939s, episode steps: 175, steps per second: 186, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.361 [-2.413, 1.019], loss: 1.620936, mean_absolute_error: 19.848337, mean_q: 40.401791\n",
      "  4978/60000: episode: 69, duration: 1.160s, episode steps: 162, steps per second: 140, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.389 [-2.436, 0.832], loss: 2.511390, mean_absolute_error: 20.522270, mean_q: 41.756424\n",
      "  5146/60000: episode: 70, duration: 1.165s, episode steps: 168, steps per second: 144, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.382 [-2.421, 1.146], loss: 2.462846, mean_absolute_error: 21.006138, mean_q: 42.721050\n",
      "  5337/60000: episode: 71, duration: 1.250s, episode steps: 191, steps per second: 153, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.334 [-2.422, 1.150], loss: 2.617030, mean_absolute_error: 21.590956, mean_q: 43.862240\n",
      "  5537/60000: episode: 72, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.202 [-1.736, 1.241], loss: 2.112552, mean_absolute_error: 22.230181, mean_q: 45.106533\n",
      "  5720/60000: episode: 73, duration: 1.889s, episode steps: 183, steps per second: 97, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.353 [-2.423, 1.004], loss: 2.638520, mean_absolute_error: 22.891306, mean_q: 46.466263\n",
      "  5920/60000: episode: 74, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.234 [-0.983, 2.056], loss: 2.482200, mean_absolute_error: 23.386896, mean_q: 47.526688\n",
      "  6117/60000: episode: 75, duration: 2.048s, episode steps: 197, steps per second: 96, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.315 [-2.405, 1.304], loss: 2.503280, mean_absolute_error: 23.811003, mean_q: 48.451469\n",
      "  6307/60000: episode: 76, duration: 1.303s, episode steps: 190, steps per second: 146, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.340 [-2.432, 1.293], loss: 1.994978, mean_absolute_error: 24.460644, mean_q: 49.731819\n",
      "  6459/60000: episode: 77, duration: 0.677s, episode steps: 152, steps per second: 225, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.419 [-2.418, 0.830], loss: 1.968494, mean_absolute_error: 24.997662, mean_q: 50.768505\n",
      "  6642/60000: episode: 78, duration: 0.816s, episode steps: 183, steps per second: 224, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.355 [-2.439, 1.193], loss: 3.438717, mean_absolute_error: 25.276810, mean_q: 51.320526\n",
      "  6813/60000: episode: 79, duration: 0.997s, episode steps: 171, steps per second: 171, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.368 [-2.406, 0.939], loss: 2.265440, mean_absolute_error: 25.949856, mean_q: 52.732643\n",
      "  6973/60000: episode: 80, duration: 0.708s, episode steps: 160, steps per second: 226, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.385 [-2.728, 0.953], loss: 2.682434, mean_absolute_error: 26.294903, mean_q: 53.388172\n",
      "  7167/60000: episode: 81, duration: 0.950s, episode steps: 194, steps per second: 204, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.329 [-2.560, 1.302], loss: 4.491245, mean_absolute_error: 26.590780, mean_q: 53.993721\n",
      "  7326/60000: episode: 82, duration: 0.894s, episode steps: 159, steps per second: 178, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.392 [-2.534, 0.918], loss: 2.204069, mean_absolute_error: 26.900747, mean_q: 54.621258\n",
      "  7514/60000: episode: 83, duration: 0.979s, episode steps: 188, steps per second: 192, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.333 [-2.428, 1.234], loss: 2.474866, mean_absolute_error: 27.296028, mean_q: 55.401981\n",
      "  7686/60000: episode: 84, duration: 0.730s, episode steps: 172, steps per second: 236, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.372 [-2.906, 0.987], loss: 2.609650, mean_absolute_error: 27.836861, mean_q: 56.478607\n",
      "  7886/60000: episode: 85, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.281 [-2.093, 0.934], loss: 2.411780, mean_absolute_error: 28.141392, mean_q: 57.093575\n",
      "  8086/60000: episode: 86, duration: 1.045s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.232 [-1.711, 0.955], loss: 3.068050, mean_absolute_error: 28.570757, mean_q: 57.885468\n",
      "  8250/60000: episode: 87, duration: 0.908s, episode steps: 164, steps per second: 181, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.388 [-2.403, 0.952], loss: 2.083945, mean_absolute_error: 29.070520, mean_q: 58.949337\n",
      "  8450/60000: episode: 88, duration: 1.028s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.227 [-1.737, 1.290], loss: 2.817474, mean_absolute_error: 29.465488, mean_q: 59.644344\n",
      "  8650/60000: episode: 89, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.093 [-1.246, 1.163], loss: 2.976490, mean_absolute_error: 29.838432, mean_q: 60.424992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8819/60000: episode: 90, duration: 1.002s, episode steps: 169, steps per second: 169, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.386 [-2.531, 1.179], loss: 2.951592, mean_absolute_error: 30.318909, mean_q: 61.339157\n",
      "  8992/60000: episode: 91, duration: 0.822s, episode steps: 173, steps per second: 211, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.365 [-2.535, 0.804], loss: 1.652319, mean_absolute_error: 30.429821, mean_q: 61.692703\n",
      "  9192/60000: episode: 92, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.320 [-2.309, 1.175], loss: 3.946754, mean_absolute_error: 30.645880, mean_q: 62.004505\n",
      "  9359/60000: episode: 93, duration: 1.545s, episode steps: 167, steps per second: 108, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.396 [-2.388, 1.008], loss: 2.427330, mean_absolute_error: 30.912056, mean_q: 62.648064\n",
      "  9532/60000: episode: 94, duration: 1.268s, episode steps: 173, steps per second: 136, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.362 [-2.288, 0.894], loss: 3.181055, mean_absolute_error: 30.920057, mean_q: 62.579334\n",
      "  9732/60000: episode: 95, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.187 [-1.444, 1.049], loss: 3.799905, mean_absolute_error: 31.506968, mean_q: 63.641167\n",
      "  9932/60000: episode: 96, duration: 1.155s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.244 [-1.784, 0.894], loss: 3.581948, mean_absolute_error: 31.801435, mean_q: 64.335556\n",
      " 10117/60000: episode: 97, duration: 1.024s, episode steps: 185, steps per second: 181, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.356 [-2.784, 1.151], loss: 3.391932, mean_absolute_error: 31.966475, mean_q: 64.678841\n",
      " 10317/60000: episode: 98, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.121 [-1.295, 1.260], loss: 3.570425, mean_absolute_error: 32.340786, mean_q: 65.396599\n",
      " 10500/60000: episode: 99, duration: 1.112s, episode steps: 183, steps per second: 165, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.348 [-2.775, 1.166], loss: 3.897998, mean_absolute_error: 32.606163, mean_q: 65.879005\n",
      " 10700/60000: episode: 100, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.319 [-2.447, 1.124], loss: 2.207119, mean_absolute_error: 32.793179, mean_q: 66.256432\n",
      " 10871/60000: episode: 101, duration: 1.099s, episode steps: 171, steps per second: 156, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.389 [-3.138, 1.273], loss: 3.863929, mean_absolute_error: 32.909206, mean_q: 66.441757\n",
      " 11042/60000: episode: 102, duration: 1.145s, episode steps: 171, steps per second: 149, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.370 [-2.592, 1.004], loss: 3.243122, mean_absolute_error: 32.908535, mean_q: 66.521271\n",
      " 11204/60000: episode: 103, duration: 1.058s, episode steps: 162, steps per second: 153, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.400 [-2.948, 1.027], loss: 2.034383, mean_absolute_error: 32.960239, mean_q: 66.655838\n",
      " 11401/60000: episode: 104, duration: 1.326s, episode steps: 197, steps per second: 149, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.342 [-2.569, 1.264], loss: 2.722896, mean_absolute_error: 33.135796, mean_q: 67.056938\n",
      " 11554/60000: episode: 105, duration: 1.102s, episode steps: 153, steps per second: 139, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.420 [-2.596, 1.211], loss: 2.437854, mean_absolute_error: 33.113316, mean_q: 67.015930\n",
      " 11710/60000: episode: 106, duration: 0.764s, episode steps: 156, steps per second: 204, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.408 [-2.773, 1.452], loss: 2.640969, mean_absolute_error: 33.654022, mean_q: 68.037094\n",
      " 11910/60000: episode: 107, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.247 [-1.832, 1.285], loss: 1.865732, mean_absolute_error: 33.633202, mean_q: 68.034393\n",
      " 12110/60000: episode: 108, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.286 [-2.020, 1.208], loss: 4.837399, mean_absolute_error: 33.926441, mean_q: 68.554596\n",
      " 12310/60000: episode: 109, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.309 [-2.220, 1.280], loss: 3.281912, mean_absolute_error: 34.239479, mean_q: 69.246193\n",
      " 12484/60000: episode: 110, duration: 1.107s, episode steps: 174, steps per second: 157, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.374 [-2.410, 1.177], loss: 3.000138, mean_absolute_error: 34.095192, mean_q: 68.964653\n",
      " 12684/60000: episode: 111, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.323 [-2.584, 1.216], loss: 3.102353, mean_absolute_error: 34.640968, mean_q: 70.062614\n",
      " 12873/60000: episode: 112, duration: 1.163s, episode steps: 189, steps per second: 162, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.352 [-2.740, 1.167], loss: 2.265792, mean_absolute_error: 34.530140, mean_q: 69.860825\n",
      " 13073/60000: episode: 113, duration: 1.054s, episode steps: 200, steps per second: 190, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.141 [-1.300, 1.400], loss: 2.010485, mean_absolute_error: 34.742996, mean_q: 70.206886\n",
      " 13257/60000: episode: 114, duration: 1.336s, episode steps: 184, steps per second: 138, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.284 [-1.887, 1.102], loss: 2.574914, mean_absolute_error: 35.038723, mean_q: 70.864769\n",
      " 13457/60000: episode: 115, duration: 1.993s, episode steps: 200, steps per second: 100, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.260 [-2.068, 1.117], loss: 3.444871, mean_absolute_error: 34.947384, mean_q: 70.508987\n",
      " 13657/60000: episode: 116, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.188 [-1.516, 1.304], loss: 2.349633, mean_absolute_error: 35.499996, mean_q: 71.669601\n",
      " 13857/60000: episode: 117, duration: 1.056s, episode steps: 200, steps per second: 189, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.332 [-2.433, 0.891], loss: 3.613740, mean_absolute_error: 35.592449, mean_q: 71.798050\n",
      " 14057/60000: episode: 118, duration: 1.125s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.336 [-2.298, 1.320], loss: 2.362496, mean_absolute_error: 35.539131, mean_q: 71.794739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14257/60000: episode: 119, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.303 [-2.119, 1.174], loss: 2.572878, mean_absolute_error: 35.663273, mean_q: 72.044075\n",
      " 14446/60000: episode: 120, duration: 1.146s, episode steps: 189, steps per second: 165, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.342 [-2.314, 1.558], loss: 2.897006, mean_absolute_error: 35.887707, mean_q: 72.354416\n",
      " 14639/60000: episode: 121, duration: 1.182s, episode steps: 193, steps per second: 163, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.348 [-2.433, 1.445], loss: 1.997161, mean_absolute_error: 35.568996, mean_q: 71.852516\n",
      " 14839/60000: episode: 122, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.105 [-1.505, 1.578], loss: 2.634878, mean_absolute_error: 35.621849, mean_q: 71.879082\n",
      " 15017/60000: episode: 123, duration: 0.944s, episode steps: 178, steps per second: 189, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.377 [-2.432, 1.003], loss: 3.175183, mean_absolute_error: 36.017868, mean_q: 72.665031\n",
      " 15217/60000: episode: 124, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.045 [-1.328, 1.040], loss: 2.142695, mean_absolute_error: 35.963184, mean_q: 72.687111\n",
      " 15417/60000: episode: 125, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.155 [-1.182, 1.071], loss: 2.726698, mean_absolute_error: 36.040550, mean_q: 72.846352\n",
      " 15617/60000: episode: 126, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.318 [-2.296, 1.319], loss: 3.791151, mean_absolute_error: 36.157909, mean_q: 72.872665\n",
      " 15817/60000: episode: 127, duration: 1.187s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.159 [-1.174, 1.490], loss: 2.665415, mean_absolute_error: 36.304794, mean_q: 73.307526\n",
      " 16017/60000: episode: 128, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.180 [-1.476, 1.356], loss: 2.844110, mean_absolute_error: 36.520096, mean_q: 73.668854\n",
      " 16217/60000: episode: 129, duration: 1.650s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.017 [-1.144, 1.050], loss: 3.953750, mean_absolute_error: 36.906288, mean_q: 74.443207\n",
      " 16417/60000: episode: 130, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.196 [-0.799, 1.806], loss: 3.888419, mean_absolute_error: 37.217823, mean_q: 74.963623\n",
      " 16597/60000: episode: 131, duration: 1.051s, episode steps: 180, steps per second: 171, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.372 [-2.601, 1.397], loss: 5.753331, mean_absolute_error: 37.093185, mean_q: 74.705688\n",
      " 16797/60000: episode: 132, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.218 [-1.363, 2.022], loss: 1.925623, mean_absolute_error: 37.015400, mean_q: 74.780197\n",
      " 16997/60000: episode: 133, duration: 1.171s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.234 [-1.639, 1.096], loss: 2.823697, mean_absolute_error: 37.047455, mean_q: 74.900703\n",
      " 17174/60000: episode: 134, duration: 0.970s, episode steps: 177, steps per second: 182, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.378 [-2.545, 0.744], loss: 4.259038, mean_absolute_error: 37.043629, mean_q: 74.773643\n",
      " 17367/60000: episode: 135, duration: 1.039s, episode steps: 193, steps per second: 186, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.299 [-2.057, 1.132], loss: 2.182127, mean_absolute_error: 37.148590, mean_q: 74.959618\n",
      " 17564/60000: episode: 136, duration: 0.849s, episode steps: 197, steps per second: 232, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.335 [-2.803, 0.944], loss: 2.550537, mean_absolute_error: 37.056778, mean_q: 74.729683\n",
      " 17764/60000: episode: 137, duration: 1.084s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.121 [-1.131, 1.179], loss: 2.560260, mean_absolute_error: 37.647606, mean_q: 75.929611\n",
      " 17964/60000: episode: 138, duration: 1.072s, episode steps: 200, steps per second: 187, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.226 [-2.017, 1.259], loss: 4.513179, mean_absolute_error: 37.449657, mean_q: 75.405869\n",
      " 18162/60000: episode: 139, duration: 1.033s, episode steps: 198, steps per second: 192, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.320 [-0.811, 2.519], loss: 2.426194, mean_absolute_error: 37.334511, mean_q: 75.356873\n",
      " 18362/60000: episode: 140, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.101 [-1.145, 1.263], loss: 2.335233, mean_absolute_error: 37.035702, mean_q: 74.776604\n",
      " 18562/60000: episode: 141, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.146 [-1.328, 1.251], loss: 2.191935, mean_absolute_error: 37.529018, mean_q: 75.899727\n",
      " 18762/60000: episode: 142, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.248 [-1.847, 1.059], loss: 2.353884, mean_absolute_error: 37.640072, mean_q: 75.981659\n",
      " 18909/60000: episode: 143, duration: 1.094s, episode steps: 147, steps per second: 134, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.324 [-0.745, 2.054], loss: 2.008485, mean_absolute_error: 37.892174, mean_q: 76.529778\n",
      " 19109/60000: episode: 144, duration: 1.160s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.065 [-1.734, 1.437], loss: 2.857591, mean_absolute_error: 37.810562, mean_q: 76.268333\n",
      " 19309/60000: episode: 145, duration: 1.082s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.135 [-1.170, 1.488], loss: 2.083979, mean_absolute_error: 38.293217, mean_q: 77.302834\n",
      " 19509/60000: episode: 146, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.064 [-1.043, 1.266], loss: 3.187865, mean_absolute_error: 38.398876, mean_q: 77.298500\n",
      " 19671/60000: episode: 147, duration: 0.931s, episode steps: 162, steps per second: 174, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.310 [-1.251, 2.152], loss: 1.932451, mean_absolute_error: 37.942211, mean_q: 76.510666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19871/60000: episode: 148, duration: 1.038s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-1.199, 1.510], loss: 3.443650, mean_absolute_error: 38.714245, mean_q: 77.978409\n",
      " 20052/60000: episode: 149, duration: 0.939s, episode steps: 181, steps per second: 193, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.348 [-1.436, 2.738], loss: 4.629624, mean_absolute_error: 38.132523, mean_q: 76.908417\n",
      " 20252/60000: episode: 150, duration: 1.198s, episode steps: 200, steps per second: 167, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.031 [-1.159, 1.269], loss: 5.357780, mean_absolute_error: 38.177036, mean_q: 76.817322\n",
      " 20424/60000: episode: 151, duration: 0.807s, episode steps: 172, steps per second: 213, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.349 [-0.975, 2.938], loss: 4.063147, mean_absolute_error: 38.521435, mean_q: 77.673950\n",
      " 20624/60000: episode: 152, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-1.565, 1.456], loss: 4.200523, mean_absolute_error: 38.630238, mean_q: 77.856018\n",
      " 20781/60000: episode: 153, duration: 0.772s, episode steps: 157, steps per second: 203, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.400 [-0.760, 2.442], loss: 4.434140, mean_absolute_error: 38.455235, mean_q: 77.631981\n",
      " 20981/60000: episode: 154, duration: 1.048s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.202 [-0.960, 1.983], loss: 3.069367, mean_absolute_error: 38.460129, mean_q: 77.744133\n",
      " 21150/60000: episode: 155, duration: 1.044s, episode steps: 169, steps per second: 162, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.362 [-0.967, 2.768], loss: 3.514580, mean_absolute_error: 38.670429, mean_q: 78.040581\n",
      " 21350/60000: episode: 156, duration: 1.017s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-1.029, 1.001], loss: 3.420010, mean_absolute_error: 38.441669, mean_q: 77.707375\n",
      " 21512/60000: episode: 157, duration: 0.796s, episode steps: 162, steps per second: 203, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.378 [-1.006, 2.561], loss: 4.375195, mean_absolute_error: 38.790226, mean_q: 78.384438\n",
      " 21712/60000: episode: 158, duration: 1.125s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.294, 1.041], loss: 6.053835, mean_absolute_error: 38.251728, mean_q: 77.111694\n",
      " 21891/60000: episode: 159, duration: 0.948s, episode steps: 179, steps per second: 189, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.343 [-1.219, 2.810], loss: 5.029751, mean_absolute_error: 38.410957, mean_q: 77.407707\n",
      " 22091/60000: episode: 160, duration: 1.175s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.007 [-1.177, 1.175], loss: 3.380195, mean_absolute_error: 38.700882, mean_q: 78.275452\n",
      " 22223/60000: episode: 161, duration: 0.801s, episode steps: 132, steps per second: 165, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.446 [-0.639, 2.748], loss: 5.972700, mean_absolute_error: 38.923466, mean_q: 78.557983\n",
      " 22366/60000: episode: 162, duration: 0.940s, episode steps: 143, steps per second: 152, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.406 [-1.342, 2.810], loss: 6.764282, mean_absolute_error: 38.517235, mean_q: 77.703156\n",
      " 22532/60000: episode: 163, duration: 1.226s, episode steps: 166, steps per second: 135, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.367 [-0.990, 2.528], loss: 2.807641, mean_absolute_error: 38.642456, mean_q: 78.073044\n",
      " 22688/60000: episode: 164, duration: 0.865s, episode steps: 156, steps per second: 180, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.373 [-1.291, 2.628], loss: 6.470684, mean_absolute_error: 38.427685, mean_q: 77.480675\n",
      " 22888/60000: episode: 165, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.063 [-1.254, 1.299], loss: 3.208082, mean_absolute_error: 38.669525, mean_q: 78.034172\n",
      " 23057/60000: episode: 166, duration: 1.033s, episode steps: 169, steps per second: 164, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.365 [-1.182, 2.532], loss: 2.137760, mean_absolute_error: 38.945068, mean_q: 78.658859\n",
      " 23257/60000: episode: 167, duration: 1.005s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.281 [-1.076, 2.969], loss: 1.720866, mean_absolute_error: 38.862804, mean_q: 78.499924\n",
      " 23423/60000: episode: 168, duration: 0.859s, episode steps: 166, steps per second: 193, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.360 [-1.187, 2.993], loss: 5.031312, mean_absolute_error: 39.319691, mean_q: 79.315971\n",
      " 23623/60000: episode: 169, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.223 [-1.521, 2.572], loss: 5.838874, mean_absolute_error: 39.076630, mean_q: 78.753860\n",
      " 23823/60000: episode: 170, duration: 0.917s, episode steps: 200, steps per second: 218, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.188 [-1.485, 1.854], loss: 3.171251, mean_absolute_error: 39.094086, mean_q: 78.897568\n",
      " 23996/60000: episode: 171, duration: 1.310s, episode steps: 173, steps per second: 132, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.388 [-2.789, 1.094], loss: 3.786271, mean_absolute_error: 38.744041, mean_q: 78.135895\n",
      " 24151/60000: episode: 172, duration: 1.147s, episode steps: 155, steps per second: 135, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.392 [-0.680, 2.603], loss: 3.349383, mean_absolute_error: 38.881691, mean_q: 78.377869\n",
      " 24351/60000: episode: 173, duration: 1.146s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.058 [-1.456, 1.200], loss: 5.596776, mean_absolute_error: 39.097435, mean_q: 78.852898\n",
      " 24531/60000: episode: 174, duration: 1.078s, episode steps: 180, steps per second: 167, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.269 [-1.302, 2.136], loss: 3.446757, mean_absolute_error: 38.897491, mean_q: 78.482185\n",
      " 24719/60000: episode: 175, duration: 1.071s, episode steps: 188, steps per second: 176, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.330 [-0.894, 2.772], loss: 4.441120, mean_absolute_error: 38.916500, mean_q: 78.491997\n",
      " 24919/60000: episode: 176, duration: 0.956s, episode steps: 200, steps per second: 209, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.196 [-1.035, 2.004], loss: 2.971940, mean_absolute_error: 38.917095, mean_q: 78.564735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25086/60000: episode: 177, duration: 0.985s, episode steps: 167, steps per second: 169, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.362 [-1.075, 2.788], loss: 5.215618, mean_absolute_error: 38.766438, mean_q: 78.074104\n",
      " 25286/60000: episode: 178, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-1.442, 1.325], loss: 3.559634, mean_absolute_error: 38.894726, mean_q: 78.441055\n",
      " 25466/60000: episode: 179, duration: 0.995s, episode steps: 180, steps per second: 181, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.347 [-0.881, 2.744], loss: 4.637564, mean_absolute_error: 39.207375, mean_q: 78.983917\n",
      " 25652/60000: episode: 180, duration: 0.943s, episode steps: 186, steps per second: 197, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.324 [-0.765, 2.771], loss: 4.510697, mean_absolute_error: 38.849396, mean_q: 78.349449\n",
      " 25852/60000: episode: 181, duration: 1.007s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.137 [-1.319, 1.679], loss: 4.562850, mean_absolute_error: 39.019012, mean_q: 78.580818\n",
      " 26052/60000: episode: 182, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.123 [-1.084, 1.437], loss: 4.638271, mean_absolute_error: 38.924072, mean_q: 78.539757\n",
      " 26248/60000: episode: 183, duration: 1.006s, episode steps: 196, steps per second: 195, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.321 [-1.289, 2.769], loss: 3.803846, mean_absolute_error: 39.077820, mean_q: 78.860863\n",
      " 26432/60000: episode: 184, duration: 1.040s, episode steps: 184, steps per second: 177, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.335 [-1.094, 2.889], loss: 4.081380, mean_absolute_error: 39.268604, mean_q: 79.290939\n",
      " 26611/60000: episode: 185, duration: 1.076s, episode steps: 179, steps per second: 166, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.347 [-0.802, 2.776], loss: 3.061490, mean_absolute_error: 39.358940, mean_q: 79.486588\n",
      " 26789/60000: episode: 186, duration: 1.007s, episode steps: 178, steps per second: 177, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.344 [-1.060, 2.937], loss: 3.613865, mean_absolute_error: 39.081867, mean_q: 78.943787\n",
      " 26989/60000: episode: 187, duration: 1.000s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.175 [-1.193, 2.057], loss: 4.275439, mean_absolute_error: 39.354687, mean_q: 79.495316\n",
      " 27163/60000: episode: 188, duration: 0.928s, episode steps: 174, steps per second: 188, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.350 [-1.085, 2.917], loss: 2.833388, mean_absolute_error: 39.021580, mean_q: 78.796082\n",
      " 27336/60000: episode: 189, duration: 0.999s, episode steps: 173, steps per second: 173, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.345 [-1.094, 3.118], loss: 5.250767, mean_absolute_error: 39.229778, mean_q: 79.075401\n",
      " 27531/60000: episode: 190, duration: 0.950s, episode steps: 195, steps per second: 205, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.322 [-0.943, 2.769], loss: 2.852012, mean_absolute_error: 39.104080, mean_q: 78.884071\n",
      " 27689/60000: episode: 191, duration: 0.840s, episode steps: 158, steps per second: 188, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.295 [-0.953, 2.044], loss: 1.390290, mean_absolute_error: 39.348949, mean_q: 79.505661\n",
      " 27870/60000: episode: 192, duration: 0.783s, episode steps: 181, steps per second: 231, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.336 [-1.161, 3.153], loss: 2.740741, mean_absolute_error: 39.315174, mean_q: 79.362976\n",
      " 28070/60000: episode: 193, duration: 0.914s, episode steps: 200, steps per second: 219, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.319 [-1.424, 2.569], loss: 4.196676, mean_absolute_error: 39.557587, mean_q: 79.972565\n",
      " 28263/60000: episode: 194, duration: 1.073s, episode steps: 193, steps per second: 180, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.298 [-0.795, 2.376], loss: 2.640455, mean_absolute_error: 39.171612, mean_q: 79.187012\n",
      " 28463/60000: episode: 195, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.188 [-0.781, 1.900], loss: 3.283204, mean_absolute_error: 39.678909, mean_q: 80.165817\n",
      " 28663/60000: episode: 196, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.158 [-1.024, 1.707], loss: 2.814749, mean_absolute_error: 39.753780, mean_q: 80.195618\n",
      " 28842/60000: episode: 197, duration: 1.413s, episode steps: 179, steps per second: 127, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.340 [-1.097, 2.935], loss: 4.975672, mean_absolute_error: 39.368206, mean_q: 79.371887\n",
      " 29023/60000: episode: 198, duration: 1.201s, episode steps: 181, steps per second: 151, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.314 [-0.898, 2.339], loss: 2.729243, mean_absolute_error: 39.376232, mean_q: 79.699028\n",
      " 29216/60000: episode: 199, duration: 0.932s, episode steps: 193, steps per second: 207, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.271 [-0.894, 2.243], loss: 5.230582, mean_absolute_error: 39.085670, mean_q: 78.882362\n",
      " 29416/60000: episode: 200, duration: 1.036s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.211 [-0.899, 1.646], loss: 3.798051, mean_absolute_error: 39.593189, mean_q: 80.039574\n",
      " 29616/60000: episode: 201, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.306 [-1.238, 2.449], loss: 3.596405, mean_absolute_error: 39.574207, mean_q: 80.117477\n",
      " 29803/60000: episode: 202, duration: 1.163s, episode steps: 187, steps per second: 161, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.311 [-1.001, 2.351], loss: 2.967059, mean_absolute_error: 39.826004, mean_q: 80.611580\n",
      " 29987/60000: episode: 203, duration: 0.949s, episode steps: 184, steps per second: 194, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.262 [-1.021, 2.223], loss: 2.933982, mean_absolute_error: 39.670074, mean_q: 80.182976\n",
      " 30156/60000: episode: 204, duration: 1.040s, episode steps: 169, steps per second: 163, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.234 [-1.278, 2.104], loss: 5.879012, mean_absolute_error: 39.653576, mean_q: 79.999077\n",
      " 30344/60000: episode: 205, duration: 1.291s, episode steps: 188, steps per second: 146, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.206 [-1.443, 2.226], loss: 3.170445, mean_absolute_error: 39.789467, mean_q: 80.476997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30532/60000: episode: 206, duration: 0.777s, episode steps: 188, steps per second: 242, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.208 [-1.603, 2.302], loss: 2.303527, mean_absolute_error: 40.188740, mean_q: 81.286888\n",
      " 30732/60000: episode: 207, duration: 0.858s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.202 [-1.372, 2.405], loss: 3.803193, mean_absolute_error: 39.853279, mean_q: 80.527756\n",
      " 30932/60000: episode: 208, duration: 0.832s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-1.163, 1.453], loss: 3.470919, mean_absolute_error: 40.239456, mean_q: 81.375923\n",
      " 31132/60000: episode: 209, duration: 0.813s, episode steps: 200, steps per second: 246, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.115 [-0.620, 1.144], loss: 1.861237, mean_absolute_error: 40.421516, mean_q: 81.870262\n",
      " 31332/60000: episode: 210, duration: 0.829s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.052 [-0.587, 0.536], loss: 4.940541, mean_absolute_error: 40.685211, mean_q: 82.184013\n",
      " 31532/60000: episode: 211, duration: 0.829s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.747, 0.907], loss: 3.652528, mean_absolute_error: 40.952599, mean_q: 82.832169\n",
      " 31732/60000: episode: 212, duration: 0.831s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.864, 0.726], loss: 5.874291, mean_absolute_error: 41.192841, mean_q: 83.230919\n",
      " 31932/60000: episode: 213, duration: 0.839s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.790, 0.822], loss: 6.991926, mean_absolute_error: 41.815163, mean_q: 84.376930\n",
      " 32132/60000: episode: 214, duration: 0.818s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-1.146, 1.513], loss: 4.469501, mean_absolute_error: 41.581928, mean_q: 83.977776\n",
      " 32332/60000: episode: 215, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-0.762, 0.689], loss: 4.115720, mean_absolute_error: 41.933350, mean_q: 84.767029\n",
      " 32532/60000: episode: 216, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.993, 0.834], loss: 5.056674, mean_absolute_error: 42.303604, mean_q: 85.529709\n",
      " 32732/60000: episode: 217, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.935, 0.987], loss: 4.120555, mean_absolute_error: 42.460617, mean_q: 85.999512\n",
      " 32932/60000: episode: 218, duration: 1.008s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.032 [-0.670, 0.832], loss: 6.458269, mean_absolute_error: 42.849854, mean_q: 86.850647\n",
      " 33132/60000: episode: 219, duration: 0.991s, episode steps: 200, steps per second: 202, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.893, 1.109], loss: 4.915447, mean_absolute_error: 43.230431, mean_q: 87.652481\n",
      " 33332/60000: episode: 220, duration: 1.055s, episode steps: 200, steps per second: 190, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.668, 0.760], loss: 5.862675, mean_absolute_error: 43.134953, mean_q: 87.238617\n",
      " 33532/60000: episode: 221, duration: 0.899s, episode steps: 200, steps per second: 222, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.741, 0.888], loss: 5.409407, mean_absolute_error: 43.719296, mean_q: 88.373024\n",
      " 33732/60000: episode: 222, duration: 1.173s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.029 [-0.692, 0.872], loss: 4.482685, mean_absolute_error: 43.766171, mean_q: 88.687859\n",
      " 33932/60000: episode: 223, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.646, 0.713], loss: 4.502188, mean_absolute_error: 44.533062, mean_q: 90.238762\n",
      " 34132/60000: episode: 224, duration: 0.875s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.024 [-0.672, 0.873], loss: 12.572624, mean_absolute_error: 44.772385, mean_q: 90.476555\n",
      " 34332/60000: episode: 225, duration: 0.840s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.030 [-0.877, 0.875], loss: 13.744210, mean_absolute_error: 44.987217, mean_q: 90.693810\n",
      " 34532/60000: episode: 226, duration: 0.829s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-0.751, 0.872], loss: 4.896590, mean_absolute_error: 45.211056, mean_q: 91.501663\n",
      " 34732/60000: episode: 227, duration: 0.830s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.024 [-0.845, 0.844], loss: 6.688575, mean_absolute_error: 46.071232, mean_q: 93.352364\n",
      " 34932/60000: episode: 228, duration: 0.811s, episode steps: 200, steps per second: 247, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.791, 0.600], loss: 5.043756, mean_absolute_error: 45.696922, mean_q: 92.785355\n",
      " 35132/60000: episode: 229, duration: 1.055s, episode steps: 200, steps per second: 190, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.027 [-0.660, 0.881], loss: 14.051892, mean_absolute_error: 46.201992, mean_q: 93.450882\n",
      " 35332/60000: episode: 230, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.024 [-0.873, 0.854], loss: 13.189527, mean_absolute_error: 46.842880, mean_q: 94.750320\n",
      " 35532/60000: episode: 231, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-1.023, 1.376], loss: 9.935477, mean_absolute_error: 46.591377, mean_q: 94.337090\n",
      " 35732/60000: episode: 232, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.022 [-0.600, 0.750], loss: 12.878572, mean_absolute_error: 46.949989, mean_q: 95.056976\n",
      " 35932/60000: episode: 233, duration: 0.855s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-1.005, 1.174], loss: 17.475643, mean_absolute_error: 47.763954, mean_q: 96.394493\n",
      " 36132/60000: episode: 234, duration: 0.847s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.023 [-0.756, 0.632], loss: 14.961222, mean_absolute_error: 47.804031, mean_q: 96.451843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36332/60000: episode: 235, duration: 0.838s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-0.817, 0.799], loss: 8.699410, mean_absolute_error: 47.563148, mean_q: 96.235138\n",
      " 36532/60000: episode: 236, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.716, 0.667], loss: 10.286829, mean_absolute_error: 48.480637, mean_q: 98.071312\n",
      " 36732/60000: episode: 237, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-0.791, 0.754], loss: 13.554796, mean_absolute_error: 48.739403, mean_q: 98.399002\n",
      " 36932/60000: episode: 238, duration: 0.829s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.032 [-0.836, 0.742], loss: 16.465443, mean_absolute_error: 48.643967, mean_q: 98.329720\n",
      " 37132/60000: episode: 239, duration: 0.832s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-0.935, 0.793], loss: 13.916983, mean_absolute_error: 49.139374, mean_q: 99.240158\n",
      " 37332/60000: episode: 240, duration: 0.852s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.828, 0.811], loss: 8.522675, mean_absolute_error: 49.952965, mean_q: 101.041954\n",
      " 37532/60000: episode: 241, duration: 0.846s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.977, 0.880], loss: 9.303232, mean_absolute_error: 50.048264, mean_q: 101.286995\n",
      " 37732/60000: episode: 242, duration: 0.840s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.828, 1.153], loss: 15.321680, mean_absolute_error: 50.346172, mean_q: 101.498528\n",
      " 37932/60000: episode: 243, duration: 0.862s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.030 [-0.888, 0.885], loss: 11.690094, mean_absolute_error: 50.941669, mean_q: 102.997833\n",
      " 38132/60000: episode: 244, duration: 0.835s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.032 [-1.001, 0.867], loss: 6.971979, mean_absolute_error: 51.492504, mean_q: 103.994278\n",
      " 38332/60000: episode: 245, duration: 0.860s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-1.038, 0.906], loss: 13.170901, mean_absolute_error: 51.479557, mean_q: 103.812698\n",
      " 38532/60000: episode: 246, duration: 0.850s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.757, 0.943], loss: 11.919560, mean_absolute_error: 51.897251, mean_q: 104.887131\n",
      " 38732/60000: episode: 247, duration: 0.842s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.030 [-0.936, 0.922], loss: 18.131357, mean_absolute_error: 52.138634, mean_q: 105.097122\n",
      " 38932/60000: episode: 248, duration: 0.830s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.028 [-0.832, 0.751], loss: 14.386255, mean_absolute_error: 52.528744, mean_q: 105.729355\n",
      " 39132/60000: episode: 249, duration: 0.817s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.033 [-0.958, 1.116], loss: 14.783828, mean_absolute_error: 52.462521, mean_q: 105.770569\n",
      " 39332/60000: episode: 250, duration: 0.842s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.759, 1.138], loss: 14.648128, mean_absolute_error: 52.878105, mean_q: 106.669670\n",
      " 39532/60000: episode: 251, duration: 0.857s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.748, 0.848], loss: 17.119753, mean_absolute_error: 53.138130, mean_q: 107.157120\n",
      " 39732/60000: episode: 252, duration: 0.827s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-0.856, 1.146], loss: 11.419308, mean_absolute_error: 52.903114, mean_q: 106.663643\n",
      " 39932/60000: episode: 253, duration: 0.848s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.976, 0.845], loss: 14.066364, mean_absolute_error: 53.432362, mean_q: 107.797424\n",
      " 40132/60000: episode: 254, duration: 0.841s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.037 [-0.968, 0.841], loss: 18.851171, mean_absolute_error: 53.876202, mean_q: 108.548790\n",
      " 40332/60000: episode: 255, duration: 0.833s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.042 [-0.986, 0.924], loss: 10.571329, mean_absolute_error: 54.956085, mean_q: 110.867889\n",
      " 40532/60000: episode: 256, duration: 0.838s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-1.117, 0.991], loss: 9.394729, mean_absolute_error: 54.561474, mean_q: 109.948708\n",
      " 40732/60000: episode: 257, duration: 0.827s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.918, 0.807], loss: 17.396156, mean_absolute_error: 55.019398, mean_q: 110.704414\n",
      " 40932/60000: episode: 258, duration: 0.821s, episode steps: 200, steps per second: 244, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-1.096, 0.978], loss: 25.089838, mean_absolute_error: 54.861786, mean_q: 110.062248\n",
      " 41132/60000: episode: 259, duration: 0.856s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-1.040, 0.998], loss: 18.519096, mean_absolute_error: 54.822140, mean_q: 110.352791\n",
      " 41332/60000: episode: 260, duration: 0.844s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.071 [-0.913, 1.148], loss: 11.158583, mean_absolute_error: 54.722622, mean_q: 110.437698\n",
      " 41532/60000: episode: 261, duration: 0.866s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.076 [-0.735, 0.837], loss: 14.915801, mean_absolute_error: 55.505539, mean_q: 111.839767\n",
      " 41732/60000: episode: 262, duration: 0.841s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-1.027, 0.929], loss: 10.585732, mean_absolute_error: 55.811310, mean_q: 112.633064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41932/60000: episode: 263, duration: 0.841s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-1.132, 1.004], loss: 24.184238, mean_absolute_error: 55.820736, mean_q: 112.449219\n",
      " 42132/60000: episode: 264, duration: 0.842s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.071 [-0.949, 1.089], loss: 14.577908, mean_absolute_error: 56.100372, mean_q: 113.000725\n",
      " 42332/60000: episode: 265, duration: 0.835s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.827, 1.118], loss: 14.016582, mean_absolute_error: 56.432686, mean_q: 113.692406\n",
      " 42532/60000: episode: 266, duration: 0.845s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-1.126, 1.382], loss: 15.447055, mean_absolute_error: 56.589676, mean_q: 113.967186\n",
      " 42732/60000: episode: 267, duration: 0.839s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-1.229, 1.130], loss: 10.416678, mean_absolute_error: 56.193359, mean_q: 113.346863\n",
      " 42932/60000: episode: 268, duration: 0.830s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.898, 0.963], loss: 20.625462, mean_absolute_error: 56.977417, mean_q: 114.820312\n",
      " 43132/60000: episode: 269, duration: 0.859s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.091 [-1.267, 1.361], loss: 18.077122, mean_absolute_error: 56.819981, mean_q: 114.319489\n",
      " 43332/60000: episode: 270, duration: 0.846s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.802, 1.231], loss: 17.985018, mean_absolute_error: 56.928680, mean_q: 114.702286\n",
      " 43532/60000: episode: 271, duration: 0.859s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.859, 0.943], loss: 18.293522, mean_absolute_error: 57.218048, mean_q: 115.239822\n",
      " 43732/60000: episode: 272, duration: 0.837s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-1.037, 1.181], loss: 14.842568, mean_absolute_error: 57.052948, mean_q: 114.942383\n",
      " 43932/60000: episode: 273, duration: 0.828s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-1.445, 1.301], loss: 21.869614, mean_absolute_error: 57.563778, mean_q: 115.637978\n",
      " 44132/60000: episode: 274, duration: 0.856s, episode steps: 200, steps per second: 234, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-1.168, 1.115], loss: 10.661278, mean_absolute_error: 57.873848, mean_q: 116.504036\n",
      " 44332/60000: episode: 275, duration: 0.862s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.142 [-0.874, 0.978], loss: 9.649091, mean_absolute_error: 57.940010, mean_q: 116.670021\n",
      " 44532/60000: episode: 276, duration: 0.839s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.122 [-0.951, 0.973], loss: 13.659607, mean_absolute_error: 57.767643, mean_q: 116.392960\n",
      " 44732/60000: episode: 277, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.157 [-0.907, 1.095], loss: 12.860487, mean_absolute_error: 57.993183, mean_q: 116.995331\n",
      " 44932/60000: episode: 278, duration: 0.866s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.173 [-0.905, 0.993], loss: 17.253910, mean_absolute_error: 57.710365, mean_q: 116.405975\n",
      " 45132/60000: episode: 279, duration: 0.863s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.222 [-1.026, 1.099], loss: 16.304163, mean_absolute_error: 58.445919, mean_q: 117.928223\n",
      " 45332/60000: episode: 280, duration: 0.823s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.182 [-1.007, 1.107], loss: 17.854095, mean_absolute_error: 58.514805, mean_q: 117.715965\n",
      " 45532/60000: episode: 281, duration: 0.827s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.203 [-1.057, 1.185], loss: 23.333765, mean_absolute_error: 58.884064, mean_q: 118.386917\n",
      " 45711/60000: episode: 282, duration: 0.771s, episode steps: 179, steps per second: 232, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.226 [-1.241, 1.322], loss: 24.426453, mean_absolute_error: 58.694668, mean_q: 117.965324\n",
      " 45906/60000: episode: 283, duration: 0.824s, episode steps: 195, steps per second: 237, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.346 [-1.013, 2.037], loss: 20.401186, mean_absolute_error: 58.346333, mean_q: 117.400459\n",
      " 46041/60000: episode: 284, duration: 0.572s, episode steps: 135, steps per second: 236, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.254 [-1.075, 1.489], loss: 28.907183, mean_absolute_error: 58.107876, mean_q: 116.647499\n",
      " 46218/60000: episode: 285, duration: 0.790s, episode steps: 177, steps per second: 224, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.222 [-1.007, 1.175], loss: 21.367626, mean_absolute_error: 58.672916, mean_q: 118.026497\n",
      " 46363/60000: episode: 286, duration: 0.610s, episode steps: 145, steps per second: 238, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.268 [-0.984, 1.281], loss: 19.277121, mean_absolute_error: 58.435562, mean_q: 117.657799\n",
      " 46530/60000: episode: 287, duration: 0.694s, episode steps: 167, steps per second: 241, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.243 [-1.015, 1.379], loss: 27.383623, mean_absolute_error: 58.546570, mean_q: 117.701485\n",
      " 46702/60000: episode: 288, duration: 0.736s, episode steps: 172, steps per second: 234, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.224 [-0.770, 1.215], loss: 13.858228, mean_absolute_error: 58.882313, mean_q: 118.751419\n",
      " 46864/60000: episode: 289, duration: 0.697s, episode steps: 162, steps per second: 232, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.368 [-0.911, 1.901], loss: 16.475576, mean_absolute_error: 58.560089, mean_q: 117.887657\n",
      " 46997/60000: episode: 290, duration: 0.575s, episode steps: 133, steps per second: 231, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.265 [-1.246, 1.499], loss: 20.822947, mean_absolute_error: 58.370277, mean_q: 117.274544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47169/60000: episode: 291, duration: 1.031s, episode steps: 172, steps per second: 167, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.310 [-1.336, 1.594], loss: 24.552502, mean_absolute_error: 58.331913, mean_q: 117.064224\n",
      " 47353/60000: episode: 292, duration: 2.920s, episode steps: 184, steps per second: 63, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.197 [-0.928, 1.291], loss: 17.603958, mean_absolute_error: 57.882122, mean_q: 116.400749\n",
      " 47486/60000: episode: 293, duration: 0.791s, episode steps: 133, steps per second: 168, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.261 [-1.229, 1.360], loss: 17.591984, mean_absolute_error: 58.563999, mean_q: 117.873932\n",
      " 47669/60000: episode: 294, duration: 1.013s, episode steps: 183, steps per second: 181, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.230 [-0.914, 1.177], loss: 26.543171, mean_absolute_error: 57.887520, mean_q: 116.254608\n",
      " 47841/60000: episode: 295, duration: 1.321s, episode steps: 172, steps per second: 130, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.220 [-0.839, 1.183], loss: 14.281633, mean_absolute_error: 57.918854, mean_q: 116.499931\n",
      " 48006/60000: episode: 296, duration: 0.878s, episode steps: 165, steps per second: 188, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.282 [-0.887, 1.450], loss: 15.275490, mean_absolute_error: 58.505089, mean_q: 117.696617\n",
      " 48152/60000: episode: 297, duration: 0.793s, episode steps: 146, steps per second: 184, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.373 [-1.101, 1.880], loss: 25.024515, mean_absolute_error: 57.744381, mean_q: 115.907883\n",
      " 48297/60000: episode: 298, duration: 0.769s, episode steps: 145, steps per second: 188, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.347 [-0.952, 1.653], loss: 18.930494, mean_absolute_error: 57.648941, mean_q: 115.932014\n",
      " 48464/60000: episode: 299, duration: 0.918s, episode steps: 167, steps per second: 182, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.399 [-1.384, 2.248], loss: 27.981298, mean_absolute_error: 57.593433, mean_q: 115.705933\n",
      " 48626/60000: episode: 300, duration: 1.404s, episode steps: 162, steps per second: 115, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.260 [-1.186, 1.286], loss: 11.102074, mean_absolute_error: 57.744129, mean_q: 116.367981\n",
      " 48794/60000: episode: 301, duration: 1.435s, episode steps: 168, steps per second: 117, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.414 [-1.439, 2.432], loss: 22.179604, mean_absolute_error: 57.548065, mean_q: 115.624489\n",
      " 48962/60000: episode: 302, duration: 1.615s, episode steps: 168, steps per second: 104, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.213 [-1.048, 1.517], loss: 23.990885, mean_absolute_error: 57.457478, mean_q: 115.257729\n",
      " 49128/60000: episode: 303, duration: 1.122s, episode steps: 166, steps per second: 148, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.244 [-1.196, 1.461], loss: 18.143862, mean_absolute_error: 56.991375, mean_q: 114.620415\n",
      " 49282/60000: episode: 304, duration: 0.715s, episode steps: 154, steps per second: 215, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.287 [-0.932, 1.464], loss: 22.448919, mean_absolute_error: 56.500969, mean_q: 113.560143\n",
      " 49435/60000: episode: 305, duration: 0.912s, episode steps: 153, steps per second: 168, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.336 [-0.964, 1.779], loss: 9.378855, mean_absolute_error: 56.363319, mean_q: 113.545227\n",
      " 49572/60000: episode: 306, duration: 0.654s, episode steps: 137, steps per second: 209, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.371 [-1.187, 1.803], loss: 18.639174, mean_absolute_error: 56.011395, mean_q: 112.459953\n",
      " 49678/60000: episode: 307, duration: 0.453s, episode steps: 106, steps per second: 234, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.325 [-0.981, 1.548], loss: 5.802551, mean_absolute_error: 56.112370, mean_q: 113.014076\n",
      " 49824/60000: episode: 308, duration: 0.670s, episode steps: 146, steps per second: 218, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.245 [-1.197, 1.404], loss: 6.882084, mean_absolute_error: 55.850567, mean_q: 112.458862\n",
      " 49999/60000: episode: 309, duration: 0.824s, episode steps: 175, steps per second: 212, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.236 [-0.787, 1.313], loss: 19.809881, mean_absolute_error: 55.870880, mean_q: 112.074417\n",
      " 50199/60000: episode: 310, duration: 0.846s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-1.057, 0.856], loss: 6.396310, mean_absolute_error: 55.634640, mean_q: 112.021484\n",
      " 50399/60000: episode: 311, duration: 0.828s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.116 [-0.823, 1.306], loss: 11.286066, mean_absolute_error: 55.256142, mean_q: 111.087990\n",
      " 50599/60000: episode: 312, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.008 [-1.144, 1.016], loss: 14.179984, mean_absolute_error: 54.873985, mean_q: 110.254662\n",
      " 50799/60000: episode: 313, duration: 0.870s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.027 [-1.020, 1.020], loss: 12.214950, mean_absolute_error: 55.021709, mean_q: 110.528694\n",
      " 50999/60000: episode: 314, duration: 0.903s, episode steps: 200, steps per second: 221, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.032 [-0.956, 1.162], loss: 16.131201, mean_absolute_error: 54.833706, mean_q: 110.003693\n",
      " 51144/60000: episode: 315, duration: 0.726s, episode steps: 145, steps per second: 200, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.238 [-1.095, 1.351], loss: 22.504065, mean_absolute_error: 54.636093, mean_q: 109.296951\n",
      " 51344/60000: episode: 316, duration: 1.522s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.323 [-1.099, 2.234], loss: 7.467213, mean_absolute_error: 53.447857, mean_q: 107.357086\n",
      " 51544/60000: episode: 317, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.044 [-1.012, 1.517], loss: 13.974993, mean_absolute_error: 53.595528, mean_q: 107.432907\n",
      " 51744/60000: episode: 318, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.030 [-0.982, 0.983], loss: 17.835567, mean_absolute_error: 53.211502, mean_q: 106.607330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51944/60000: episode: 319, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.332 [-0.861, 2.315], loss: 12.244852, mean_absolute_error: 53.340523, mean_q: 106.882721\n",
      " 52144/60000: episode: 320, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.028 [-1.474, 1.430], loss: 21.545027, mean_absolute_error: 52.900059, mean_q: 105.667992\n",
      " 52264/60000: episode: 321, duration: 0.615s, episode steps: 120, steps per second: 195, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.428 [-1.113, 1.910], loss: 7.633661, mean_absolute_error: 52.842766, mean_q: 106.182854\n",
      " 52448/60000: episode: 322, duration: 1.151s, episode steps: 184, steps per second: 160, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.279 [-1.333, 1.799], loss: 15.522013, mean_absolute_error: 52.194164, mean_q: 104.566414\n",
      " 52648/60000: episode: 323, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.252 [-1.326, 1.826], loss: 9.434587, mean_absolute_error: 52.045265, mean_q: 104.541298\n",
      " 52778/60000: episode: 324, duration: 0.747s, episode steps: 130, steps per second: 174, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.353 [-0.964, 1.847], loss: 10.605876, mean_absolute_error: 52.047829, mean_q: 104.475975\n",
      " 52944/60000: episode: 325, duration: 1.025s, episode steps: 166, steps per second: 162, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.382 [-1.363, 2.726], loss: 20.235868, mean_absolute_error: 51.502518, mean_q: 103.039612\n",
      " 53097/60000: episode: 326, duration: 1.012s, episode steps: 153, steps per second: 151, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.283 [-1.082, 1.581], loss: 12.955449, mean_absolute_error: 50.832119, mean_q: 101.971733\n",
      " 53200/60000: episode: 327, duration: 0.636s, episode steps: 103, steps per second: 162, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.372 [-0.802, 1.513], loss: 13.064872, mean_absolute_error: 51.255749, mean_q: 102.763878\n",
      " 53400/60000: episode: 328, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.034 [-1.735, 2.083], loss: 17.761780, mean_absolute_error: 50.829071, mean_q: 101.646698\n",
      " 53600/60000: episode: 329, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.033 [-1.147, 1.442], loss: 13.764517, mean_absolute_error: 50.293594, mean_q: 100.784691\n",
      " 53800/60000: episode: 330, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.031 [-1.389, 1.552], loss: 16.264944, mean_absolute_error: 50.045147, mean_q: 100.102615\n",
      " 54000/60000: episode: 331, duration: 1.147s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.021 [-1.402, 1.711], loss: 18.406410, mean_absolute_error: 49.782555, mean_q: 99.494942\n",
      " 54200/60000: episode: 332, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.208 [-1.445, 1.494], loss: 9.866391, mean_absolute_error: 49.277435, mean_q: 98.715057\n",
      " 54400/60000: episode: 333, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.022 [-1.355, 1.645], loss: 10.849205, mean_absolute_error: 48.911201, mean_q: 97.960838\n",
      " 54600/60000: episode: 334, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.036 [-1.384, 1.717], loss: 7.348264, mean_absolute_error: 48.645523, mean_q: 97.505219\n",
      " 54800/60000: episode: 335, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-1.443, 1.511], loss: 9.032507, mean_absolute_error: 48.832260, mean_q: 97.626663\n",
      " 55000/60000: episode: 336, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.037 [-1.397, 1.690], loss: 12.652414, mean_absolute_error: 49.199306, mean_q: 98.251190\n",
      " 55200/60000: episode: 337, duration: 0.968s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.030 [-1.323, 1.567], loss: 10.742594, mean_absolute_error: 47.951950, mean_q: 95.801132\n",
      " 55300/60000: episode: 338, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.399 [-0.909, 2.202], loss: 18.838371, mean_absolute_error: 47.681118, mean_q: 95.132011\n",
      " 55422/60000: episode: 339, duration: 0.540s, episode steps: 122, steps per second: 226, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.343 [-1.056, 1.502], loss: 9.631981, mean_absolute_error: 47.706558, mean_q: 95.370972\n",
      " 55559/60000: episode: 340, duration: 0.696s, episode steps: 137, steps per second: 197, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.483 [-1.440, 2.423], loss: 21.672577, mean_absolute_error: 47.826969, mean_q: 95.197281\n",
      " 55759/60000: episode: 341, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.063 [-1.517, 1.598], loss: 13.397928, mean_absolute_error: 46.680859, mean_q: 93.216415\n",
      " 55959/60000: episode: 342, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.035 [-1.494, 1.603], loss: 12.140193, mean_absolute_error: 46.785587, mean_q: 93.376572\n",
      " 56159/60000: episode: 343, duration: 1.148s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.044 [-1.562, 1.776], loss: 12.295056, mean_absolute_error: 46.837177, mean_q: 93.566750\n",
      " 56359/60000: episode: 344, duration: 0.832s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-1.572, 1.767], loss: 6.922015, mean_absolute_error: 46.137413, mean_q: 92.319298\n",
      " 56559/60000: episode: 345, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.043 [-1.331, 1.553], loss: 12.891531, mean_absolute_error: 45.983749, mean_q: 91.865173\n",
      " 56759/60000: episode: 346, duration: 0.823s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.026 [-1.594, 1.761], loss: 8.323778, mean_absolute_error: 45.723511, mean_q: 91.425751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56903/60000: episode: 347, duration: 0.596s, episode steps: 144, steps per second: 242, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.229 [-1.379, 1.369], loss: 9.912623, mean_absolute_error: 46.189026, mean_q: 92.241035\n",
      " 57103/60000: episode: 348, duration: 0.837s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.059 [-1.479, 1.578], loss: 15.702417, mean_absolute_error: 45.211514, mean_q: 90.110977\n",
      " 57303/60000: episode: 349, duration: 0.835s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.048 [-1.495, 1.664], loss: 12.863317, mean_absolute_error: 45.199547, mean_q: 90.285645\n",
      " 57503/60000: episode: 350, duration: 0.835s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.053 [-1.101, 1.169], loss: 17.623615, mean_absolute_error: 45.007084, mean_q: 89.872353\n",
      " 57703/60000: episode: 351, duration: 0.825s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.043 [-1.145, 1.077], loss: 11.506548, mean_absolute_error: 44.838001, mean_q: 89.607872\n",
      " 57903/60000: episode: 352, duration: 0.839s, episode steps: 200, steps per second: 238, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.160 [-1.496, 1.471], loss: 12.071812, mean_absolute_error: 44.148926, mean_q: 88.206886\n",
      " 58103/60000: episode: 353, duration: 0.836s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.048 [-1.378, 1.430], loss: 12.496463, mean_absolute_error: 44.395298, mean_q: 88.599457\n",
      " 58303/60000: episode: 354, duration: 0.823s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.040 [-1.680, 1.937], loss: 7.285800, mean_absolute_error: 43.861427, mean_q: 87.759743\n",
      " 58503/60000: episode: 355, duration: 0.851s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.151 [-1.356, 1.323], loss: 13.951324, mean_absolute_error: 43.694473, mean_q: 87.084442\n",
      " 58703/60000: episode: 356, duration: 0.798s, episode steps: 200, steps per second: 250, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.047 [-1.513, 1.639], loss: 13.060800, mean_absolute_error: 43.702023, mean_q: 87.113106\n",
      " 58903/60000: episode: 357, duration: 0.816s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.300, 1.527], loss: 11.689103, mean_absolute_error: 43.189728, mean_q: 86.043144\n",
      " 59071/60000: episode: 358, duration: 0.692s, episode steps: 168, steps per second: 243, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.125 [-1.954, 2.116], loss: 13.686936, mean_absolute_error: 43.057240, mean_q: 85.833961\n",
      " 59271/60000: episode: 359, duration: 0.837s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.224 [-2.025, 1.662], loss: 9.396385, mean_absolute_error: 42.914806, mean_q: 85.667450\n",
      " 59471/60000: episode: 360, duration: 0.849s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.051 [-1.867, 2.043], loss: 9.887533, mean_absolute_error: 43.097523, mean_q: 86.070946\n",
      " 59552/60000: episode: 361, duration: 0.333s, episode steps: 81, steps per second: 244, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.030 [-1.979, 2.258], loss: 10.456283, mean_absolute_error: 43.427597, mean_q: 86.658630\n",
      " 59752/60000: episode: 362, duration: 0.861s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.023 [-1.447, 1.737], loss: 11.609349, mean_absolute_error: 42.926811, mean_q: 85.735771\n",
      " 59952/60000: episode: 363, duration: 0.815s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.041 [-1.938, 2.103], loss: 6.661953, mean_absolute_error: 42.852207, mean_q: 85.914413\n",
      "done, took 326.978 seconds\n",
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "abstract",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3490b432acdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartpole_nnet_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-5c16caf5bbe6>\u001b[0m in \u001b[0;36mcartpole_nnet_to_txt\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcartpole_nnet_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cartpole_nnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cartpole_nnet.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7608f1f389f5>\u001b[0m in \u001b[0;36mtrain_cartpole_nnet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'on_action_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0maxleoffset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, display)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_closed_by_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_default_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         '''\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_screens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         '''\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_default_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: abstract"
     ]
    }
   ],
   "source": [
    "weights = cartpole_nnet_to_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
