<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Solvers · NeuralVerification.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NeuralVerification.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">NeuralVerification.jl</a></li><li><a class="toctext" href="../problem/">Problem Definitions</a></li><li class="current"><a class="toctext" href>Solvers</a><ul class="internal"><li><a class="toctext" href="#Reachability-Methods-1">Reachability Methods</a></li><li><a class="toctext" href="#Primal-Optimization-Methods-1">Primal Optimization Methods</a></li><li><a class="toctext" href="#Dual-Optimization-Methods-1">Dual Optimization Methods</a></li><li><a class="toctext" href="#Search-and-Reachability-Methods-1">Search and Reachability Methods</a></li><li><a class="toctext" href="#Search-and-Optimization-Methods-1">Search and Optimization Methods</a></li></ul></li><li><a class="toctext" href="../functions/">Helper Functions</a></li><li><a class="toctext" href="../existing_implementations/">Existing Implementations</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Solvers</a></li></ul><a class="edit-page" href="https://github.com/sisl/NeuralVerification.jl/blob/master/docs/src/solvers.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Solvers</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Solvers-1" href="#Solvers-1">Solvers</a></h1><p>The three basic verification methods are &quot;reachability&quot;, &quot;optimization&quot;, and &quot;search&quot;. These are further divided into the five categories listed below. Note that all of the optimization methods use the <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP.jl</a> library.</p><ul><li><a href="#Solvers-1">Solvers</a></li><ul><li><a href="#Reachability-Methods-1">Reachability Methods</a></li><ul><li><a href="#ExactReach-1">ExactReach</a></li><li><a href="#Ai2-1">Ai2</a></li><li><a href="#MaxSens-1">MaxSens</a></li></ul><li><a href="#Primal-Optimization-Methods-1">Primal Optimization Methods</a></li><ul><li><a href="#NSVerify-1">NSVerify</a></li><li><a href="#MIPVerify-1">MIPVerify</a></li><li><a href="#ILP-1">ILP</a></li></ul><li><a href="#Dual-Optimization-Methods-1">Dual Optimization Methods</a></li><ul><li><a href="#Duality-1">Duality</a></li><li><a href="#ConvDual-1">ConvDual</a></li><li><a href="#Certify-1">Certify</a></li></ul><li><a href="#Search-and-Reachability-Methods-1">Search and Reachability Methods</a></li><ul><li><a href="#ReluVal-1">ReluVal</a></li><li><a href="#FastLin-1">FastLin</a></li><li><a href="#FastLip-1">FastLip</a></li><li><a href="#DLV-1">DLV</a></li></ul><li><a href="#Search-and-Optimization-Methods-1">Search and Optimization Methods</a></li><ul><li><a href="#Sherlock-1">Sherlock</a></li><li><a href="#BaB-1">BaB</a></li><li><a href="#Planet-1">Planet</a></li><li><a href="#Reluplex-1">Reluplex</a></li></ul></ul></ul><h2><a class="nav-anchor" id="Reachability-Methods-1" href="#Reachability-Methods-1">Reachability Methods</a></h2><p>These methods perform exact or approximate reachability analysis to determine the output set corresponding to a given input set. In addition, <code>MaxSens</code>, which computes lower and upper bounds for each layer, is called within other solver types in the form of <a href="@ref"><code>get_bounds</code></a>.</p><h3><a class="nav-anchor" id="ExactReach-1" href="#ExactReach-1">ExactReach</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.ExactReach" href="#NeuralVerification.ExactReach"><code>NeuralVerification.ExactReach</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ExactReach</code></pre><p>ExactReach performs exact reachability analysis to compute the output reachable set for a network.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: HPolytope</li><li>Output: HPolytope</li></ol><p><strong>Return</strong></p><p><code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Exact reachability analysis.</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1712.08163">W. Xiang, H.-D. Tran, and T. T. Johnson, &quot;Reachable Set Computation and Safety Verification for Neural Networks with ReLU Activations,&quot; <em>ArXiv Preprint ArXiv:1712.08163</em>, 2017.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/sisl/NeuralVerification.jl/blob/56774ae6da1e9e7e0983fddf83f93d67b94701ec/src/reachability/exactReach.jl#L1-L24">source</a></section><h3><a class="nav-anchor" id="Ai2-1" href="#Ai2-1">Ai2</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.Ai2" href="#NeuralVerification.Ai2"><code>NeuralVerification.Ai2</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Ai2</code></pre><p>Ai2 performs over-approximated reachability analysis to compute the over-approximated output reachable set for a network.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation (more activations to be supported in the future)</li><li>Input: HPolytope</li><li>Output: HPolytope</li></ol><p><strong>Return</strong></p><p><code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Reachability analysis using split and join.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p>T. Gehr, M. Mirman, D. Drashsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, &quot;Ai2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation,&quot; in <em>2018 IEEE Symposium on Security and Privacy (SP)</em>, 2018.</p></div></div><a class="source-link" target="_blank" href="https://github.com/sisl/NeuralVerification.jl/blob/56774ae6da1e9e7e0983fddf83f93d67b94701ec/src/reachability/ai2.jl#L1-L24">source</a></section><h3><a class="nav-anchor" id="MaxSens-1" href="#MaxSens-1">MaxSens</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.MaxSens" href="#NeuralVerification.MaxSens"><code>NeuralVerification.MaxSens</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MaxSens(resolution::Float64, tight::Bool)</code></pre><p>MaxSens performs over-approximated reachability analysis to compute the over-approximated output reachable set for a network.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, any activation that is monotone</li><li>Input: <code>Hyperrectangle</code> or <code>HPolytope</code></li><li>Output: <code>HPolytope</code></li></ol><p><strong>Return</strong></p><p><code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>First partition the input space into small grid cells according to <code>resolution</code>. Then use interval arithmetic to compute the reachable set for each cell. Two versions of interval arithmetic is implemented with indicator <code>tight</code>. Default <code>resolution</code> is <code>1.0</code>. Default <code>tight = false</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1708.03322">W. Xiang, H.-D. Tran, and T. T. Johnson, &quot;Output Reachable Set Estimation and Verification for Multi-Layer Neural Networks,&quot; <em>ArXiv Preprint ArXiv:1708.03322</em>, 2017.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L26">source</a></section><h2><a class="nav-anchor" id="Primal-Optimization-Methods-1" href="#Primal-Optimization-Methods-1">Primal Optimization Methods</a></h2><h4><a class="nav-anchor" id="Example-1" href="#Example-1">Example</a></h4><div><pre><code class="language-julia">nnet = read_nnet(&quot;../../examples/networks/small_nnet.nnet&quot;)
input  = Hyperrectangle([0.0], [.5])
output = HPolytope(ones(1,1), [102.5])

problem = Problem(nnet, input, output)
# set the JuMP solver with `optimizer` keyword or use default:
solver = MIPVerify(optimizer = GLPKSolverMIP())

solve(solver,  problem)</code></pre><pre><code class="language-none">AdversarialResult(:SAT, -1.0)</code></pre></div><h3><a class="nav-anchor" id="NSVerify-1" href="#NSVerify-1">NSVerify</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.NSVerify" href="#NeuralVerification.NSVerify"><code>NeuralVerification.NSVerify</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">NSVerify(optimizer, m::Float64)</code></pre><p>NSVerify finds counter examples using mixed integer linear programming.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle or hpolytope</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code></p><p><strong>Method</strong></p><p>MILP encoding (using <code>m</code>). No presolve. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>. Default <code>m</code> is <code>1000.0</code> (should be large enough to avoid approximation error).</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1706.07351">A. Lomuscio and L. Maganti, &quot;An Approach to Reachability Analysis for Feed-Forward Relu Neural Networks,&quot; <em>ArXiv Preprint ArXiv:1706.07351</em>, 2017.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L24">source</a></section><h3><a class="nav-anchor" id="MIPVerify-1" href="#MIPVerify-1">MIPVerify</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.MIPVerify" href="#NeuralVerification.MIPVerify"><code>NeuralVerification.MIPVerify</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MIPVerify(optimizer)</code></pre><p>MIPVerify computes maximum allowable disturbance using mixed integer linear programming.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>MILP encoding. Use presolve to compute a tight node-wise bounds first. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>.</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p>V. Tjeng, K. Xiao, and R. Tedrake, <a href="https://arxiv.org/abs/1711.07356">&quot;Evaluating Robustness of Neural Networks with Mixed Integer Programming,&quot; <em>ArXiv Preprint ArXiv:1711.07356</em>, 2017.</a></p><p><a href="https://github.com/vtjeng/MIPVerify.jl">https://github.com/vtjeng/MIPVerify.jl</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L27">source</a></section><h3><a class="nav-anchor" id="ILP-1" href="#ILP-1">ILP</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.ILP" href="#NeuralVerification.ILP"><code>NeuralVerification.ILP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ILP(optimizer, max_iter)</code></pre><p>ILP iteratively solves a linearized primal optimization to compute maximum allowable disturbance.  It iteratively adds the linear constraint to the problem.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>Iteratively solve a linear encoding of the problem. It only considers the linear piece of the network that has the same activation pattern as the reference input. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>. We provide both iterative method and non-iterative method to solve the LP problem. Default <code>iterative</code> is <code>true</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1605.07262">O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, and A. Criminisi, &quot;Measuring Neural Net Robustness with Constraints,&quot; in <em>Advances in Neural Information Processing Systems</em>, 2016.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L28">source</a></section><h2><a class="nav-anchor" id="Dual-Optimization-Methods-1" href="#Dual-Optimization-Methods-1">Dual Optimization Methods</a></h2><h3><a class="nav-anchor" id="Duality-1" href="#Duality-1">Duality</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.Duality" href="#NeuralVerification.Duality"><code>NeuralVerification.Duality</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Duality(optimizer)</code></pre><p>Duality uses Lagrangian relaxation to compute over-approximated bounds for a network</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, any activation function that is monotone</li><li>Input: hyperrectangle</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Lagrangian relaxation. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1803.06567">K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, &quot;A Dual Approach to Scalable Verification of Deep Networks,&quot; <em>ArXiv Preprint ArXiv:1803.06567</em>, 2018.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L24">source</a></section><h3><a class="nav-anchor" id="ConvDual-1" href="#ConvDual-1">ConvDual</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.ConvDual" href="#NeuralVerification.ConvDual"><code>NeuralVerification.ConvDual</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ConvDual</code></pre><p>ConvDual uses convex relaxation to compute over-approximated bounds for a network</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hypercube</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Convex relaxation with duality.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1711.00851">E. Wong and J. Z. Kolter, &quot;Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope,&quot; <em>ArXiv Preprint ArXiv:1711.00851</em>, 2017.</a></p><p><a href="https://github.com/locuslab/convex_adversarial">https://github.com/locuslab/convex_adversarial</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/sisl/NeuralVerification.jl/blob/56774ae6da1e9e7e0983fddf83f93d67b94701ec/src/optimization/convDual.jl#L1-L25">source</a></section><h3><a class="nav-anchor" id="Certify-1" href="#Certify-1">Certify</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.Certify" href="#NeuralVerification.Certify"><code>NeuralVerification.Certify</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Certify(optimizer)</code></pre><p>Certify uses semidefinite programming to compute over-approximated certificates for a neural network with only one hidden layer.</p><p><strong>Problem requirement</strong></p><ol><li>Network: one hidden layer, any activation that is differentiable almost everywhere whose derivative is bound by 0 and 1</li><li>Input: hypercube</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Semindefinite programming. Default <code>optimizer</code> is <code>SCSSolver()</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1801.09344">A. Raghunathan, J. Steinhardt, and P. Liang, &quot;Certified Defenses against Adversarial Examples,&quot; <em>ArXiv Preprint ArXiv:1801.09344</em>, 2018.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L24">source</a></section><h2><a class="nav-anchor" id="Search-and-Reachability-Methods-1" href="#Search-and-Reachability-Methods-1">Search and Reachability Methods</a></h2><h3><a class="nav-anchor" id="ReluVal-1" href="#ReluVal-1">ReluVal</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.ReluVal" href="#NeuralVerification.ReluVal"><code>NeuralVerification.ReluVal</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ReluVal(max_iter::Int64, tree_search::Symbol)</code></pre><p>ReluVal combines symbolic reachability analysis with iterative interval refinement to minimize over-approximation of the reachable set.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: hpolytope</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code> or <code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Symbolic reachability analysis and iterative interval refinement (search).</p><ul><li><code>max_iter</code> default <code>10</code>.</li><li><code>tree_search</code> default <code>:DFS</code> - depth first search.</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1804.10829">S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, &quot;Formal Security Analysis of Neural Networks Using Symbolic Intervals,&quot; <em>CoRR</em>, vol. abs/1804.10829, 2018. arXiv: 1804.10829.</a></p><p><a href="https://github.com/tcwangshiqi-columbia/ReluVal">https://github.com/tcwangshiqi-columbia/ReluVal</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L27">source</a></section><h3><a class="nav-anchor" id="FastLin-1" href="#FastLin-1">FastLin</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.FastLin" href="#NeuralVerification.FastLin"><code>NeuralVerification.FastLin</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">FastLin(maxIter::Int64, ϵ0::Float64, accuracy::Float64)</code></pre><p>FastLin combines reachability analysis with binary search to find maximum allowable disturbance.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hypercube</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>Reachability analysis by network approximation and binary search.</p><ul><li><code>max_iter</code> is the maximum iteration in search, default <code>10</code>;</li><li><code>ϵ0</code> is the initial search radius, default <code>100.0</code>;</li><li><code>accuracy</code> is the stopping criteria, default <code>0.1</code>;</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1804.09699">T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel, &quot;Towards Fast Computation of Certified Robustness for ReLU Networks,&quot; <em>ArXiv Preprint ArXiv:1804.09699</em>, 2018.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L26">source</a></section><h3><a class="nav-anchor" id="FastLip-1" href="#FastLip-1">FastLip</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.FastLip" href="#NeuralVerification.FastLip"><code>NeuralVerification.FastLip</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">FastLip(maxIter::Int64, ϵ0::Float64, accuracy::Float64)</code></pre><p>FastLip adds Lipschitz estimation on top of FastLin.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hypercube</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>Lipschitz estimation + FastLin. All arguments are for FastLin.</p><ul><li><code>max_iter</code> is the maximum iteration in search, default <code>10</code>;</li><li><code>ϵ0</code> is the initial search radius, default <code>100.0</code>;</li><li><code>accuracy</code> is the stopping criteria, default <code>0.1</code>;</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1804.09699">T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel, &quot;Towards Fast Computation of Certified Robustness for ReLU Networks,&quot; <em>ArXiv Preprint ArXiv:1804.09699</em>, 2018.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L26">source</a></section><h3><a class="nav-anchor" id="DLV-1" href="#DLV-1">DLV</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.DLV" href="#NeuralVerification.DLV"><code>NeuralVerification.DLV</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">DLV(ϵ::Float64)</code></pre><p>DLV searches layer by layer for counter examples in hidden layers.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, any activation (currently only support ReLU)</li><li>Input: hyperrectangle</li><li>Output: abstractpolytope</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code></p><p><strong>Method</strong></p><p>The following operations are performed layer by layer. for layer i</p><ol><li>determine a reachable set from the reachable set in layer i-1</li><li>determine a search tree in the reachable set by refining the search tree in layer i-1</li><li>Verify<ul><li>True -&gt; continue to layer i+1</li><li>False -&gt; counter example</li></ul></li></ol><p>The argument <code>ϵ</code> is the resolution of the initial search tree. Default <code>1.0</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1610.06940">X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, &quot;Safety Verification of Deep Neural Networks,&quot; in <em>International Conference on Computer Aided Verification</em>, 2017.</a></p><p><a href="https://github.com/VeriDeep/DLV">https://github.com/VeriDeep/DLV</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L32">source</a></section><h2><a class="nav-anchor" id="Search-and-Optimization-Methods-1" href="#Search-and-Optimization-Methods-1">Search and Optimization Methods</a></h2><h3><a class="nav-anchor" id="Sherlock-1" href="#Sherlock-1">Sherlock</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.Sherlock" href="#NeuralVerification.Sherlock"><code>NeuralVerification.Sherlock</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Sherlock(optimizer, ϵ::Float64)</code></pre><p>Sherlock combines local and global search to estimate the range of the output node.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation, single output</li><li>Input: hpolytope and hyperrectangle</li><li>Output: hyperrectangle (1d interval)</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code> or <code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Local search: solve a linear program to find local optima on a line segment of the piece-wise linear network. Global search: solve a feasibilty problem using MILP encoding (default calling NSVerify).</p><ul><li><code>optimizer</code> default <code>GLPKSolverMIP()</code></li><li><code>ϵ</code> is the margin for global search, default <code>0.1</code>.</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1709.09130">S. Dutta, S. Jha, S. Sanakaranarayanan, and A. Tiwari, &quot;Output Range Analysis for Deep Neural Networks,&quot; <em>ArXiv Preprint ArXiv:1709.09130</em>, 2017.</a></p><p><a href="https://github.com/souradeep-111/sherlock">https://github.com/souradeep-111/sherlock</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L28">source</a></section><h3><a class="nav-anchor" id="BaB-1" href="#BaB-1">BaB</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.BaB" href="#NeuralVerification.BaB"><code>NeuralVerification.BaB</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">BaB(optimizer, ϵ::Float64)</code></pre><p>BaB uses branch and bound to estimate the range of the output node.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation, single output</li><li>Input: hyperrectangle</li><li>Output: hyperrectangle (1d interval)</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code> or <code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Branch and bound. For branch, it uses iterative interval refinement. For bound, it computes concrete bounds by sampling, approximated bound by optimization.</p><ul><li><code>optimizer</code> default <code>GLPKSolverMIP()</code></li><li><code>ϵ</code> is the desired accurancy for termination, default <code>0.1</code>.</li></ul><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1711.00455">R. Bunel, I. Turkaslan, P. H. Torr, P. Kohli, and M. P. Kumar, &quot;A Unified View of Piecewise Linear Neural Network Verification,&quot; <em>ArXiv Preprint ArXiv:1711.00455</em>, 2017.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L27">source</a></section><h3><a class="nav-anchor" id="Planet-1" href="#Planet-1">Planet</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.Planet" href="#NeuralVerification.Planet"><code>NeuralVerification.Planet</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Planet(optimizer, eager::Bool)</code></pre><p>Planet integrates a SAT solver (<code>PicoSAT.jl</code>) to find an activation pattern that maps a feasible input to an infeasible output.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle or hpolytope</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Binary search of activations (0/1) and pruning by optimization. Our implementation is non eager.</p><ul><li><code>optimizer</code> default <code>GLPKSolverMIP()</code>;</li><li><code>eager</code> default <code>false</code>;</li></ul><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1705.01320">R. Ehlers, &quot;Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks,&quot; in <em>International Symposium on Automated Technology for Verification and Analysis</em>, 2017.</a></p><p><a href="https://github.com/progirep/planet">https://github.com/progirep/planet</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/980eda064e5a0381a6afa19046d77c1f33e1f301/base/#L0-L26">source</a></section><h3><a class="nav-anchor" id="Reluplex-1" href="#Reluplex-1">Reluplex</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NeuralVerification.Reluplex" href="#NeuralVerification.Reluplex"><code>NeuralVerification.Reluplex</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Reluplex(optimizer, eager::Bool)</code></pre><p>Reluplex uses binary tree search to find an activation pattern that maps a feasible input to an infeasible output.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code></p><p><strong>Method</strong></p><p>Binary search of activations (0/1) and pruning by optimization.</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1702.01135">G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, &quot;Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks,&quot; in <em>International Conference on Computer Aided Verification</em>, 2017.</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/sisl/NeuralVerification.jl/blob/56774ae6da1e9e7e0983fddf83f93d67b94701ec/src/satisfiability/reluplex.jl#L1-L24">source</a></section><footer><hr/><a class="previous" href="../problem/"><span class="direction">Previous</span><span class="title">Problem Definitions</span></a><a class="next" href="../functions/"><span class="direction">Next</span><span class="title">Helper Functions</span></a></footer></article></body></html>
