{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_nnet (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"util.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_nnet(\"ACASXU_nnet_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the packages that we need\n",
    "Pkg.update() \n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"GLPKMathProgInterface\")\n",
    "Pkg.add(\"Cpp\")\n",
    "Pkg.add(\"Cxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages that we need\n",
    "using JuMP\n",
    "using MathProgBase\n",
    "using GLPKMathProgInterface\n",
    "using Cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "function relu(x)\n",
    "    return max.(0.0, x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer struct \n",
    "type Layer\n",
    "    bias::Vector{Float64}\n",
    "    weights::Matrix{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network struct \n",
    "type Network\n",
    "    layers::Vector{Layer} # assuming that layers includes input & output layer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_input_constraints(inputs::Matrix{Float64}, m::Model)\n",
    "    n_inputs = len(network.layers[1,:])\n",
    "    x_1 = @variable(m, x[1:n_inputs])\n",
    "    for input in inputs # getting rows or columns?\n",
    "        @constraint(m, x_1, x_i[i] == input[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_output_constraints(outputs::Matrix{Float64}, m::Model)\n",
    "    n_layers = len(network.layers)\n",
    "    n_outputs = len(network.layers[n_layers,:])\n",
    "    x_k = @variable(m, x[1:n_outputs])\n",
    "    for output in outputs\n",
    "        @constraint(m, x_k, x_k[i] == input[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_network_constraints(network::Network, m::Model)\n",
    "    for layer in network.layers # iterate by slice\n",
    "        l = len(layer.biases)\n",
    "        x_n = @variable(m, x[1:l])\n",
    "        M = 1 # is this sufficent for ReLU constraint\n",
    "        delta = @variable(m, x[1:l], Bin)\n",
    "        @constraint(m, x_n, x[i] >= ((x_n * layer.weights) + layer.biases)[i])\n",
    "        @constraint(m, x_n, x[i] <= ((x_n * layer.weights) + layer.biases)[i] + M*delta[i])\n",
    "        @constraint(m, x_n, x[i] >= 0)\n",
    "        @constraint(m, x_n, x[i] <= M*(1 - delta[i]))  \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_constraints(inputs::Matrix{Float64}, outputs::Matrix{Float64},\n",
    "                                network::Network, m::Model)\n",
    "    add_input_constraints(inputs, m)\n",
    "    add_output_constraints(outputs, m)\n",
    "    add_network_constraints(network, m)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to clone the Reluplex repo to get the nnet reader. \n",
    "\n",
    "`https://github.com/guykatzz/ReluplexCav2017`\n",
    "\n",
    "Then, cd into the nnet folder and run the following command to compile nnet into a dynamic library:\n",
    "\n",
    "`g++ -shared -fPIC nnet.cpp -o nnet.so`\n",
    "\n",
    "We will now import the dynamic library such that we can get the network layers from the .nnet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const nnet_library_path =  pwd() * \"/ReluplexCav2017/nnet/nnet.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_lib = Libdl.dlopen(nnet_library_path, Libdl.RTLD_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cpp ccall((:load_network, nnet_lib), Float64, (Float64,), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our model and add our contraints\n",
    "model = Model(solver = GLPKSolverLP()) # use GLPK LP as our solver\n",
    "initialize_constraints(inputs, outputs, network, model)\n",
    "\n",
    "# Solve the model\n",
    "status = solve(model)\n",
    "\n",
    "# Print out the solution\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
