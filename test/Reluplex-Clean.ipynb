{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Includes while package stuff is sorted out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using MathProgBase.SolverInterface\n",
    "using GLPKMathProgInterface\n",
    "using LazySets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../src/utils/activation.jl\")\n",
    "include(\"../src/utils/network.jl\")\n",
    "include(\"../src/utils/problem.jl\")\n",
    "include(\"../src/utils/util.jl\")\n",
    "\n",
    "include(\"../src/reachability/maxSens.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{alignat*}{1}\\min\\quad & 0\\\\\n",
       "\\text{Subject to} \\quad\\end{alignat*}\n",
       " $$"
      ],
      "text/plain": [
       "Feasibility problem with:\n",
       " * 0 linear constraints\n",
       " * 0 variables\n",
       "Solver is GLPKInterfaceLP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_nnet = read_nnet(\"../examples/networks/small_nnet.txt\")\n",
    "\n",
    "A = Array{Float64, 2}(2,1)\n",
    "A[1,1] = 1\n",
    "A[2,1] = -1\n",
    "#inputSet = HPolytope(A, [1.0,3.0])\n",
    "#outputSet = HPolytope(A,[0.0,1000.0])\n",
    "\n",
    "inputSet = Hyperrectangle([1.0],[.2])\n",
    "outputSet = Hyperrectangle([2.0,], [100.0])\n",
    "\n",
    "problem = Problem(small_nnet, inputSet, outputSet)\n",
    "solver = GLPKSolverLP(method=:Exact)\n",
    "m = Model(solver=solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_nnet_vars (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function init_nnet_vars(model::Model, network::Network)\n",
    "    layers = network.layers\n",
    "    #input layer and last layer have b_vars because they are unbounded\n",
    "    b_vars = Vector{Vector{Variable}}(length(layers) + 1) # +1 for input layer\n",
    "    #f_vars are always positive and used as front for ReLUs\n",
    "    f_vars = Vector{Vector{Variable}}(length(layers) -1)\n",
    "    \n",
    "    input_layer_n = size(first(layers).weights, 2)\n",
    "    all_layers_n  = [length(l.bias) for l in layers]\n",
    "    insert!(all_layers_n, 1, input_layer_n)\n",
    "\n",
    "    for (i, n) in enumerate(all_layers_n)\n",
    "        b_vars[i] = @variable(model, [1:n]) # To do: name the variables\n",
    "        if 1 < i < length(layers) + 1\n",
    "            f_vars[i-1] = @variable(model, [1:n])\n",
    "        end\n",
    "    end\n",
    "    return b_vars, f_vars\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relu_to_fix (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function relu_to_fix(broken::Array{BitArray{1},1})\n",
    "    for (i, layer) in enumerate(broken)\n",
    "        for (j, node) in enumerate(layer)\n",
    "            if node\n",
    "                return(i, j)\n",
    "            end \n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check_broken_relus (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function check_broken_relus(bs::Array{Array{JuMP.Variable,1},1}, \n",
    "        fs::Array{Array{JuMP.Variable,1},1})\n",
    "    b_values = [getvalue(b) for b in bs[2:length(bs)-1]]\n",
    "    f_values = [getvalue(f) for f in fs]\n",
    "    \n",
    "    return [(x[2] .== 0.0) .& (x[1] .> 0.0)  .| (x[2] .> 0.0) .& (x[2] .!= x[1]) for x in zip(b_values, f_values)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function encode(model::Model, problem::Problem, relu_status::Array{Array{Int64,1},1})\n",
    "    bs, fs = init_nnet_vars(model, problem.network)\n",
    "    \n",
    "    #TEST\n",
    "    #bs = [[@variable(model, b11)],\n",
    "    #     [@variable(model, b21),@variable(m, b22)],\n",
    "    #     [@variable(model, b31),@variable(m, b32)],  \n",
    "    #     [@variable(model, b41)]]\n",
    "\n",
    "    #fs = [[@variable(model, f21),@variable(m, f22)],\n",
    "    #      [@variable(model, f31),@variable(m, f32)]]\n",
    "    \n",
    "    for (i, layer) in enumerate(problem.network.layers)\n",
    "        (W, b, act) = (layer.weights, layer.bias, layer.activation)\n",
    "        \n",
    "        #first layer is different\n",
    "        if i == 1\n",
    "            for j in 1:length(layer.bias)\n",
    "               @constraint(model, -bs[2][j] + bs[1][1]*W[j] == -b[j]) \n",
    "            end\n",
    "        elseif 1<i\n",
    "            for j in 1:length(layer.bias) # For evey node\n",
    "               # @constraint(model, -bs[i+1][j] + dot(fs[i-1],W[j,:]) == -b[j])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Adding linear constraints\n",
    "    \n",
    "    #first layer\n",
    "    bounds = get_bounds(problem)\n",
    "    #@constraint(model, bs[1] .<= bounds[1].center + bounds[1].radius)\n",
    "    #@constraint(model, bs[1] .>= bounds[1].center - bounds[1].radius)\n",
    "    \n",
    "    for i in 1:length(bs)\n",
    "        @constraint(model, bs[i] .<= bounds[i].center + bounds[i].radius)\n",
    "        @constraint(model, bs[i] .>= bounds[i].center - bounds[i].radius)\n",
    "    end\n",
    "    \n",
    "    # positivity contraint for f variables\n",
    "    for i in 1:length(fs)\n",
    "        @constraint(model, fs[i] .>= zeros(length(fs[i])))\n",
    "    end\n",
    "    \n",
    "    # relu fix constraints\n",
    "    for i in 1:length(relu_status)\n",
    "       for j in 1:length(relu_status[i])\n",
    "            if relu_status[i][j] == 1\n",
    "                @constraint(model, bs[i+1][j] == fs[i][j])\n",
    "                @constraint(model, bs[i+1][j] >= 0.0)\n",
    "            elseif relu_status[i][j] == 2\n",
    "                @constraint(model, bs[i+1][j] <= 0.0)\n",
    "                @constraint(model, fs[i][j] == 0.0)\n",
    "            end \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    @objective(m, Max, 0)\n",
    "    return (bs, fs)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that were not included "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_output_constraint (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add_input_constraint(model::Model, input::HPolytope, neuron_vars::Vector{Variable})\n",
    "    in_A,  in_b  = tosimplehrep(input)\n",
    "    @constraint(model,  in_A * neuron_vars .<= in_b)\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function add_input_constraint(model::Model, input::Hyperrectangle, neuron_vars::Vector{Variable})\n",
    "    @constraint(model,  neuron_vars .<= high(input))\n",
    "    @constraint(model,  neuron_vars .>= low(input))\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function add_output_constraint(model::Model, output::AbstractPolytope, neuron_vars::Vector{Variable})\n",
    "    out_A, out_b = tosimplehrep(output)\n",
    "    @constraint(model, out_A * neuron_vars .<= out_b)\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ReluplexState\n",
    "    model::JuMP.Model\n",
    "    b_vars::Array{Array{JuMP.Variable,1},1}\n",
    "    f_vars::Array{Array{JuMP.Variable,1},1}\n",
    "    relu_status::Array{Array{Int64,1},1}\n",
    "    relus_left_to_fix::Array{BitArray{1},1}\n",
    "    depth::Int64\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solveReluplex (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function solveReluplex(problem::Problem)\n",
    "    relu_shape = [length(x.bias) for x in problem.network.layers[1:length(problem.network.layers)-1]]\n",
    "    first_m = Model(solver = GLPKSolverLP(method=:Exact))\n",
    "    relu_status = [zeros(Int, x) for x in relu_shape]\n",
    "    first_bs, first_fs = encode(first_m, problem, relu_status)\n",
    "    relus_left_to_fix = [trues(x) for x in relu_shape]\n",
    "    firstStep = ReluplexState(first_m, first_bs, first_fs, relu_status, relus_left_to_fix, 1)\n",
    "    \n",
    "    return reluplexStep(firstStep)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reluplexStep (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reluplexStep(step::ReluplexState)\n",
    "    print(\"depth: \")\n",
    "    print(step.depth)\n",
    "    print(\"\\n\")\n",
    "    status = JuMP.solve(step.model)\n",
    "    #print(step.model)\n",
    "    \n",
    "    if status == :Optimal\n",
    "        #CHECK THAT RELUS ARE CORRECT, OTHERWISE START FIXING, CALL AGAIN\n",
    "        found_broken_relu = false\n",
    "        broken = check_broken_relus(step.b_vars, step.f_vars)\n",
    "        \n",
    "        RELUTEST = false\n",
    "        for i in 1:length(broken)\n",
    "           for j in 1:length(broken[i])\n",
    "                RELUTEST = RELUTEST | broken[i][j]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if !RELUTEST\n",
    "            print(\"No broken ReLUs - SHOULD RETURN VALUE NOW\\n\")\n",
    "            print(broken)\n",
    "            print(\"\\n input: \")\n",
    "            print(getvalue(first(step.b_vars)))\n",
    "            print(\"\\n\")\n",
    "            #return (\"SAT\", first(step.b_vars))\n",
    "            return(\"SAT\")\n",
    "        end\n",
    "                \n",
    "        for i in 1:length(broken)\n",
    "           for j in 1:length(broken[i])\n",
    "                if broken[i][j]\n",
    "                    print(\"Broken Relu: \")\n",
    "                    print(i)\n",
    "                    print(\", \")\n",
    "                    print(j)\n",
    "                    print(\"\\n\")\n",
    "                    # Found a broken ReLU\n",
    "                    found_broken_relu = true\n",
    "                    # Can still try to fix\n",
    "                    if step.relus_left_to_fix[i][j]\n",
    "                        \n",
    "                        m1 = Model(solver = GLPKSolverLP(method=:Exact))\n",
    "                        m2 = Model(solver = GLPKSolverLP(method=:Exact))\n",
    "                        \n",
    "                        relu_status1 = deepcopy(step.relu_status)\n",
    "                        relu_status2 = deepcopy(step.relu_status)\n",
    "                        \n",
    "                        new_relus_left_to_fix = deepcopy(step.relus_left_to_fix)\n",
    "                        new_relus_left_to_fix[i][j] = false\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        relu_status1[i][j] = 1\n",
    "                        bs1, fs1 = encode(m1, problem, relu_status1)\n",
    "                        newStep1 = ReluplexState(m1, bs1, fs1, relu_status1, new_relus_left_to_fix, step.depth +1)\n",
    "                        #print(\"GOT TO NEW STEP 1\")\n",
    "                        res1 = reluplexStep(newStep1)\n",
    "                        if res1 == \"SAT\"\n",
    "                            return \"SAT\"\n",
    "                        end\n",
    "                        \n",
    "                        relu_status2[i][j] = 2\n",
    "                        bs2, fs2 = encode(m2, problem, relu_status2)\n",
    "                        newStep2 = ReluplexState(m2, bs2, fs2, relu_status2, new_relus_left_to_fix, step.depth +1)\n",
    "                        \n",
    "                        #print(\"GOT TO NEW STEP 2\")\n",
    "                        res2 = reluplexStep(newStep2)\n",
    "                        if res2 == \"SAT\"\n",
    "                            return \"SAT\"\n",
    "                        end\n",
    "                        \n",
    "                        break\n",
    "                    else\n",
    "                    # No relus left to fix\n",
    "                        return \"UNSAT\"\n",
    "                    end\n",
    "                end  \n",
    "            end\n",
    "            if !found_broken_relu\n",
    "#####\n",
    "            end\n",
    "        end\n",
    "        \n",
    "    elseif status == :Infeasible\n",
    "        return \"UNSAT\"\n",
    "        \n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{alignat*}{1}\\min\\quad & 0\\\\\n",
       "\\text{Subject to} \\quad\\end{alignat*}\n",
       " $$"
      ],
      "text/plain": [
       "Feasibility problem with:\n",
       " * 0 linear constraints\n",
       " * 0 variables\n",
       "Solver is GLPKInterfaceLP"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_nnet = read_nnet(\"../examples/networks/small_nnet.txt\")\n",
    "\n",
    "A = Array{Float64, 2}(2,1)\n",
    "A[1,1] = 1\n",
    "A[2,1] = -1\n",
    "#inputSet = HPolytope(A, [1.0,3.0])\n",
    "#outputSet = HPolytope(A,[0.0,1000.0])\n",
    "\n",
    "inputSet = Hyperrectangle([10.0],[1.0])\n",
    "outputSet = Hyperrectangle([0.0,], [.1])\n",
    "\n",
    "problem = Problem(small_nnet, inputSet, outputSet)\n",
    "solver = GLPKSolverLP(method=:Exact)\n",
    "m = Model(solver=solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mNot solved to optimality, status: Infeasible\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mInfeasibility ray (Farkas proof) not available\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"UNSAT\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = solveReluplex(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I dont understand why the return values are not being called or stopping the recursion, the previous prints clearly show that the SAT line is reached"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
