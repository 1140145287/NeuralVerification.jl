{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mUpdating METADATA...\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mComputing changes...\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mNo packages to install, update or remove\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage JuMP is already installed\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage GLPKMathProgInterface is already installed\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage Cpp is already installed\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage Cxx is already installed\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "Pkg.update() \n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"GLPKMathProgInterface\")\n",
    "Pkg.add(\"Cpp\")\n",
    "Pkg.add(\"Cxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using MathProgBase\n",
    "using GLPKMathProgInterface\n",
    "using Cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"util.jl\")\n",
    "include(\"network.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(Layer[Layer([1.5; 1.5], [1.0; 1.0]), Layer([2.5; 2.5], [2.0 2.0; 2.0 2.0]), Layer([3.5], [3.0 3.0])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_nnet(\"small_nnet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(Layer[Layer([0.22763; -0.188762; … ; -0.44755; 0.175917], [0.0540062 0.0540062 … -0.180027 0.242194; -1.12374 -1.12374 … -0.00917929 0.055623; … ; -1.57631 -1.57631 … -0.00832176 -0.0926607; 0.620367 0.620367 … -0.0146141 0.262575]), Layer([0.08355; -0.0213261; … ; -0.426623; 0.780925], [-0.184202 -0.184202 … 0.337388 0.395671; 0.0171639 0.0171639 … -0.0371157 0.0196442; … ; -1.24508 -1.24508 … 0.646698 -0.159838; -0.348713 -0.348713 … -0.60238 -0.142155]), Layer([-0.408486; 1.06875; … ; -0.464336; 0.114195], [0.146062 0.146062 … -1.15657 0.515279; -1.159 -1.159 … -2.33211 -0.517898; … ; -1.14982 -1.14982 … -1.17371 0.414968; -0.684643 -0.684643 … -0.064733 -0.186417]), Layer([1.48416; -0.0392727; … ; 0.0762837; -0.216393], [-0.775044 -0.775044 … 0.00288769 -0.0587614; -0.289933 -0.289933 … 0.0289771 0.0943521; … ; 0.124486 0.124486 … -0.0140547 0.0703901; 0.0862763 0.0862763 … 0.0219731 -0.0403008]), Layer([0.343884; -0.148704; … ; -0.331041; -0.654165], [-0.169473 -0.169473 … -0.0158796 0.271254; 0.143577 0.143577 … -0.00813101 -0.193434; … ; 0.233909 0.233909 … -0.0408955 -1.33666; 0.19239 0.19239 … 0.0402966 -0.291227]), Layer([-0.430061; -0.0245742; … ; -0.114347; 0.121706], [0.0198897 0.0198897 … -0.556117 -0.321128; -0.12478 -0.12478 … 0.0824242 0.0293015; … ; -0.283984 -0.283984 … 0.0293453 -0.0325373; 0.0114592 0.0114592 … -0.0156882 -0.00548695]), Layer([-0.0102815; -0.0158668; … ; -0.015336; -0.0148281], [-0.0010049 -0.0010049 … -0.00114841 0.0116412; -0.00160923 -0.00160923 … -0.0515162 0.011273; … ; 0.00386038 0.00386038 … -0.0582468 -0.0100225; 0.032974 0.032974 … -0.00340823 0.0212921])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_nnet(\"ACASXU_nnet_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_input_constraints(inputs::Matrix{Float64}, m::Model)\n",
    "    n_inputs = len(network.layers[1,:])\n",
    "    x_1 = @variable(m, x[1:n_inputs])\n",
    "    for input in inputs # getting rows or columns?\n",
    "        @constraint(m, x_1, x_i[i] == input[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_output_constraints(outputs::Matrix{Float64}, m::Model)\n",
    "    n_layers = len(network.layers)\n",
    "    n_outputs = len(network.layers[n_layers,:])\n",
    "    x_k = @variable(m, x[1:n_outputs])\n",
    "    for output in outputs\n",
    "        @constraint(m, x_k, x_k[i] == input[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_network_constraints(network::Network, m::Model)\n",
    "    for layer in network.layers # iterate by slice\n",
    "        l = len(layer.biases)\n",
    "        x_n = @variable(m, x[1:l])\n",
    "        M = 1 # is this sufficent for ReLU constraint\n",
    "        delta = @variable(m, x[1:l], Bin)\n",
    "        @constraint(m, x_n, x[i] >= ((x_n * layer.weights) + layer.biases)[i])\n",
    "        @constraint(m, x_n, x[i] <= ((x_n * layer.weights) + layer.biases)[i] + M*delta[i])\n",
    "        @constraint(m, x_n, x[i] >= 0)\n",
    "        @constraint(m, x_n, x[i] <= M*(1 - delta[i]))  \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_constraints(inputs::Matrix{Float64}, outputs::Matrix{Float64},\n",
    "                                network::Network, m::Model)\n",
    "    add_input_constraints(inputs, m)\n",
    "    #add_output_constraints(outputs, m)\n",
    "    #add_network_constraints(network, m)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to clone the Reluplex repo to get the nnet reader. \n",
    "\n",
    "`https://github.com/guykatzz/ReluplexCav2017`\n",
    "\n",
    "Then, cd into the nnet folder and run the following command to compile nnet into a dynamic library:\n",
    "\n",
    "`g++ -shared -fPIC nnet.cpp -o nnet.so`\n",
    "\n",
    "We will now import the dynamic library such that we can get the network layers from the .nnet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const nnet_library_path =  pwd() * \"/ReluplexCav2017/nnet/nnet.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_lib = Libdl.dlopen(nnet_library_path, Libdl.RTLD_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cpp ccall((:load_network, nnet_lib), Float64, (Float64,), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our model and add our contraints\n",
    "model = Model(solver = GLPKSolverLP()) # use GLPK LP as our solver\n",
    "initialize_constraints(inputs, outputs, network, model)\n",
    "\n",
    "# Solve the model\n",
    "status = solve(model)\n",
    "\n",
    "# Print out the solution\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
